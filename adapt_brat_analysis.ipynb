{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmljson import badgerfish as bf\n",
    "from xmljson import cobra as ca \n",
    "from xml.etree.ElementTree import fromstring\n",
    "import json\n",
    "from json import dumps\n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/douglasmiranda/5127251\n",
    "def find(key, dictionary):\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            for result in find(key, v):\n",
    "                yield result\n",
    "        elif isinstance(v, list):\n",
    "            for d in v:\n",
    "                for result in find(key, d):\n",
    "                    yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_sofa(lines):   \n",
    "    \n",
    "    # get text block for subject of analysis \n",
    "    data = dumps(ca.data(fromstring(lines[0])))\n",
    "    d = json.loads(data)\n",
    "\n",
    "    # print to get all output of file\n",
    "    #print(d)\n",
    "\n",
    "    xmi = d.get(\"{http://www.omg.org/XMI}XMI\").get(\"children\")\n",
    "\n",
    "    for t in xmi:\n",
    "        #t.get(\"{http:///uima/cas.ecore}Sofa\")\n",
    "        if t.get('{http:///uima/cas.ecore}Sofa'):\n",
    "            u = t.get('{http:///uima/cas.ecore}Sofa')\n",
    "            #print(u['attributes']['sofaString'])\n",
    "            if list(find('sofaString', u)):\n",
    "                text = u \n",
    "            #print(text)\n",
    "    return text, xmi\n",
    "\n",
    "# use to get text by given span                               \n",
    "def myprint(d, begin, end):\n",
    "    i = 0\n",
    "    out = ''\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            myprint(v, begin, end)\n",
    "            return myprint(v, begin, end)\n",
    "        elif i == 3 and k != 'mimeType':\n",
    "            if v:\n",
    "                return v[begin:end]\n",
    "        i += 1\n",
    "        \n",
    "# test slicing from text\n",
    "#v = myprint(text, 0, 1540)\n",
    "#print(v)\n",
    "\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationSystems(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.annotation_type_system = ['http:///org/metamap/uima/ts.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/syntax.ecore',\n",
    "                                     'http:///org/apache/uima/ruta/type.ecore',\n",
    "                                     'http:///edu/uth/clamp/nlp/typesystem.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/textsem.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/textspan.ecore',\n",
    "                                     'http:///biomedicus/v2.ecore']\n",
    "\n",
    "        self.annotation_type_relations = ['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA',\n",
    "                                     '{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument',\n",
    "                                     '{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']\n",
    "\n",
    "        self.annotation_relation_attributes = ['ClampRelationUIMA', 'SemanticRoleRelation']\n",
    "        \n",
    "        self.biomedicus_types = ['Acronym',\n",
    "                         'DayOfWeek',\n",
    "                         'DictionaryTerm',\n",
    "                         'Fuzzy',\n",
    "                         'Historical',\n",
    "                         'IndefiniteQuantifierCue',\n",
    "                         'ModificationCue', \n",
    "                         #'Negated',\n",
    "                         'NormForm', \n",
    "                         'Number',\n",
    "                         'NumberRange',\n",
    "                         'OtherAcronymSense',\n",
    "                         'ParseToken',\n",
    "                         'SeasonWord', \n",
    "                         'StandaloneQuantifier',\n",
    "                         'TemporalPhrase',\n",
    "                         'TextData', \n",
    "                         'TimeUnits',\n",
    "                         'UmlsConcept',\n",
    "                         'YearNumber']\n",
    "        \n",
    "        self.clamp_types = ['All',\n",
    "                             'Any', \n",
    "                             'BaseToken',\n",
    "                             #'CW',\n",
    "                             'Chunk',\n",
    "                             'ClampNameEntityUIMA',\n",
    "                             #'ClampRelationUIMA', # TODO with NN\n",
    "                             #'ConllDependencyNode',\n",
    "                             'NUM',\n",
    "                             'RutaBasic',\n",
    "                             #'SPACE',\n",
    "                             #'SPECIAL',\n",
    "                             #'SW',\n",
    "                             'Segment',\n",
    "                             'Sentence',\n",
    "                             'TokenSeed',\n",
    "                             'W']\n",
    "        \n",
    "        self.ctakes_types = ['AnatomicalSiteMention',\n",
    "                             'ConllDependencyNode',\n",
    "                             'ContractionToken',\n",
    "                             'DateAnnotation',\n",
    "                             'DiseaseDisorderMention',\n",
    "                             'EntityMention',\n",
    "                             'EventMention',\n",
    "                             'FractionAnnotation',\n",
    "                             'IdentifiedAnnotation',\n",
    "                             'MeasurementAnnotation',\n",
    "                             'MedicationMention',\n",
    "                             'NumeToken',\n",
    "                             'Predicate',\n",
    "                             'ProcedureMention',\n",
    "                             'RangeAnnotation',\n",
    "                             'RomanNumeralAnnotation',\n",
    "                             'SemanticArgument', # TODO with NN\n",
    "                             #'SemanticRoleRelation',\n",
    "                             'Sentence',\n",
    "                             'SignSymptomMention',\n",
    "                             'UmlsConcept',\n",
    "                             'WordToken']\n",
    "        \n",
    "        self.metamap_types = ['AcronymAbbrev',\n",
    "                             'Annotation',\n",
    "                             'AnnotationBase',\n",
    "                             'Candidate',\n",
    "                             #'Negation'#,\n",
    "                             'Phrase',\n",
    "                             'Span',\n",
    "                             'Utterance']\n",
    "        \n",
    "        self.amicus_types = ['AnatomicalSiteMention',\n",
    "                             'Candidate',\n",
    "                             'Chunk',\n",
    "                             'IndefiniteQuantifierCue',\n",
    "                             #'MedicationMention',\n",
    "                             'Number',\n",
    "                             'Phrase',\n",
    "                             'Predicate',\n",
    "                             'SemanticArgument',\n",
    "                             'SignSymptomMention',\n",
    "                             'StandalonQuantifier',\n",
    "                             'UMLSConcept']\n",
    "        \n",
    "        self.amicus_type = ['IndefiniteQuantifierCue','StandaloneQuantifier','Number']\n",
    "        \n",
    "        \n",
    "    def get_system_type(self, system):\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == 'amicus':\n",
    "            types = self.amicus_types\n",
    "\n",
    "        return types\n",
    "\n",
    "\n",
    "def get_system_annotations(xmi, system, case, text, type_of_analysis=None): # use type_of_analysis to control flow for relationship annotations\n",
    "    \n",
    "    annSys = AnnotationSystems()\n",
    "    \n",
    "    types = annSys.get_system_type(system)\n",
    "    \n",
    "    annotation_out = []\n",
    "    \n",
    "    for x in xmi:\n",
    "        for t in types:\n",
    "            for ats in annSys.annotation_type_system:\n",
    "\n",
    "                if list(find('{' + ats + '}' + t, x)):\n",
    "                    \n",
    "                    if type_of_analysis is not None and t in annSys.annotation_relation_attributes: \n",
    "                        # ['ClampRelationUIMA', 'SemanticRoleRelation']:\n",
    "                        if t == 'ClampRelationUIMA':\n",
    "                             #'Relation', need to resolve linkage by semantic argument and predicate ids\n",
    "                             # link each by 'relation' \n",
    "                             # <textsem:SemanticRoleRelation argument=\"34379\" category=\"A1\" conditional=\"false\" confidence=\"0.0\" discoveryTechnique=\"0\" id=\"0\" polarity=\"0\" predicate=\"34372\" uncertainty=\"0\" xmi:id=\"34385\"/>\n",
    "                             # <textsem:SemanticArgument begin=\"1400\" end=\"1403\" label=\"A1\" relation=\"34385\" sofa=\"1\" xmi:id=\"34379\"/>\n",
    "                             # <textsem:Predicate begin=\"1385\" end=\"1396\" frameSet=\"extricate.01\" relations=\"34385\" sofa=\"1\" xmi:id=\"34372\"/>\n",
    "                            #print(t['{' + ats + '}' + token]['attributes']['entTo'])\n",
    "                            #print(t['{' + ats + '}' + token]['attributes']['entFrom'])\n",
    "\n",
    "                            endTo = x['{' + ats + '}' + t]['attributes']['entTo']\n",
    "                            endFrom = x['{' + ats + '}' + t]['attributes']['entFrom']\n",
    "\n",
    "                            #print('{0}, {1}'.format('PARENT',  t['{' + ats + '}' + token]['attributes']))\n",
    "                            for x1 in xmi:\n",
    "                                if list(find('{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA', x1)):\n",
    "                                    if x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['{http://www.omg.org/XMI}id'] == endTo:\n",
    "                                        #print(x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endTo', endTo, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                        x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endTo', endTo, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                    if x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['{http://www.omg.org/XMI}id'] == endFrom:\n",
    "                                        #print(x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endFrom', endFrom, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                        x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endFrom', endFrom, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                        if t == 'SemanticRoleRelation':\n",
    "                             #'Relation', need to resolve linkage by semantic argument and predicate ids\n",
    "                             # link each by 'relation' \n",
    "                             # <textsem:SemanticRoleRelation argument=\"34379\" category=\"A1\" conditional=\"false\" confidence=\"0.0\" discoveryTechnique=\"0\" id=\"0\" polarity=\"0\" predicate=\"34372\" uncertainty=\"0\" xmi:id=\"34385\"/>\n",
    "                             # <textsem:SemanticArgument begin=\"1400\" end=\"1403\" label=\"A1\" relation=\"34385\" sofa=\"1\" xmi:id=\"34379\"/>\n",
    "                             # <textsem:Predicate begin=\"1385\" end=\"1396\" frameSet=\"extricate.01\" relations=\"34385\" sofa=\"1\" xmi:id=\"34372\"/>\n",
    "#                             print(t['{' + ats + '}' + token]['attributes']['argument'])\n",
    "#                             print(t['{' + ats + '}' + token]['attributes']['predicate'])\n",
    "#                             print(t['{' + ats + '}' + token]['attributes'])\n",
    "\n",
    "                            argument = t['{' + ats + '}' + t]['attributes']['argument']\n",
    "                            predicate = t['{' + ats + '}' + t]['attributes']['predicate']\n",
    "\n",
    "                            for x1 in xmi:\n",
    "                                if list(find('{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument', x1)):\n",
    "                                    if x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['{http://www.omg.org/XMI}id'] == argument:\n",
    "                                        #print(x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', argument, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['end']))\n",
    "                                        x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', argument, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['end']))\n",
    "                            for x1 in xmi:\n",
    "                                 if list(find('{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate', x)):\n",
    "                                    if x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['{http://www.omg.org/XMI}id'] == predicate:\n",
    "                                        #print(x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', predicate, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['end']))\n",
    "                                        x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', predicate, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['end']))\n",
    "                    elif t not in annSys.annotation_relation_attributes: # ['ClampRelationUIMA', 'SemanticRoleRelation']:\n",
    "                        #if (x.get('{http:///org/metamap/uima/ts.ecore}Phrase')):\n",
    "                            #print(x.get('{http:///org/metamap/uima/ts.ecore}Phrase'))\n",
    "                            #print(x['{' + ats + '}' + t]['attributes'])\n",
    "                        begin = x['{' + ats + '}' + t]['attributes']['begin']\n",
    "                        end = x['{' + ats + '}' + t]['attributes']['end']\n",
    "                        d = {'system': system, 'type': t, 'begin': begin, 'end': end, 'text': myprint(text, int(begin), int(end)), 'case': case}\n",
    "                        \n",
    "                        if d not in annotation_out:\n",
    "                            annotation_out.append(d)\n",
    "\n",
    "    df = pd.DataFrame(annotation_out)\n",
    "    # examine output from parsed xmi\n",
    "    #print(df)\n",
    "    return df, annotation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brat\n",
    "# https://stackoverflow.com/questions/15325182/how-to-filter-rows-in-pandas-by-regex\n",
    "\n",
    "# entity = test[test.brat_id.str.startswith(\"T\")]\n",
    "# relation = test[test.brat_id.str.startswith(\"R\")]\n",
    "# attribute = test[test.brat_id.str.startswith(\"A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(string):\n",
    "    pattern = re.compile(r'\\s(\\d+).*?(\\d+$)')\n",
    "    match = re.search(pattern, string)\n",
    "    return match.groups()\n",
    "\n",
    "def get_entity_type(string):\n",
    "    return string.split()[0]\n",
    "\n",
    "def get_relation_type(string):\n",
    "    out = string.split()\n",
    "    return out[0]\n",
    "\n",
    "def get_relation_entities(string):\n",
    "    out = string.split()[1:3]\n",
    "    return (out[0].split(':')[1], out[1].split(':')[1])\n",
    "    \n",
    "def get_attribute_type(string):\n",
    "    out = string.split()\n",
    "    return out[0]\n",
    "\n",
    "def get_attribute_entity(string):\n",
    "    out = string.split()[1:2]\n",
    "    return out[0]\n",
    "\n",
    "def get_attribute_degree(string):\n",
    "    if len(string.split()) > 2:\n",
    "        out = string.split()[2:3]\n",
    "        return out[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_annotations(test):\n",
    "\n",
    "    entity = test[test.brat_id.str.startswith(\"T\")]\n",
    "    relation = test[test.brat_id.str.startswith(\"R\")]\n",
    "    attribute = test[test.brat_id.str.startswith(\"A\")]\n",
    "    \n",
    "    # segregate by entity, relation, annotation\n",
    "    #print(test[test.entity_span.notnull()])\n",
    "    test['entity_span'] = entity['brat_mapping'].apply(get_span)\n",
    "    test['entity_type'] = entity['brat_mapping'].apply(get_entity_type)\n",
    "\n",
    "    test['relation_type'] = relation['brat_mapping'].apply(get_relation_type)\n",
    "    test['relation_entities'] = relation['brat_mapping'].apply(get_relation_entities)\n",
    "\n",
    "    test['attribute_type'] = attribute['brat_mapping'].apply(get_attribute_type)\n",
    "    test['attribute_entity'] = attribute['brat_mapping'].apply(get_attribute_entity)\n",
    "    test['attribute_degree'] = attribute['brat_mapping'].apply(get_attribute_degree)\n",
    "\n",
    "    # get entities\n",
    "    comp = test[test.entity_span.notnull()]\n",
    "    #print(comp)\n",
    "    cols_to_keep = ['text', 'entity_span', 'brat_id', 'entity_type', 'case']\n",
    "    comp = comp[cols_to_keep]\n",
    "    #print(comp)\n",
    "    #print(comp['entity_span'].apply(pd.Series))\n",
    "    \n",
    "    span = comp['entity_span'].apply(pd.Series)\n",
    "    span.columns = ['begin', 'end']\n",
    "    #print(span)\n",
    "    span_comp = span.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "    cols_to_keep = ['text', 'begin', 'end', 'brat_id', 'case']\n",
    "    span_comp = span_comp[cols_to_keep]\n",
    "    #print(span_comp)\n",
    "    \n",
    "    return test, span, comp, span_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relations from .ann df\n",
    "def get_gold_relations(test, span_comp):\n",
    "    comp_r = test[test.relation_entities.notnull()]\n",
    "    cols_to_keep = ['relation_type', 'relation_entities', 'brat_id', 'case']\n",
    "    comp_r = comp_r[cols_to_keep]\n",
    "\n",
    "    span_r = comp_r['relation_entities'].apply(pd.Series)\n",
    "    span_r.columns = ['entity1_r', 'entity2_r']\n",
    "\n",
    "    span_comp_r = span_r.merge(comp_r, how='inner', left_index=True, right_index=True)\n",
    "    cols_to_keep = ['relation_type', 'relation_entities','entity1_r', 'entity2_r', 'case']\n",
    "    span_comp_r = span_comp_r[cols_to_keep]\n",
    "\n",
    "    # get entities for relationship \n",
    "    entity_1r = span_comp_r.merge(span_comp, how='left', left_on=['entity1_r', 'case'], right_on=['brat_id', 'case'])\n",
    "    entity_1r = entity_1r.rename(columns={'text': 'text_entity1', 'begin': 'entity1_begin', 'end': 'entity1_end', 'brat_id': 'entity1', 'case_y': 'case'})\n",
    "    \n",
    "    entity_2r = span_comp_r.merge(span_comp, how='left', left_on=['entity2_r', 'case'], right_on=['brat_id', 'case'])\n",
    "    entity_2r = entity_2r.rename(columns={'text': 'text_entity2', 'begin': 'entity2_begin', 'end': 'entity2_end', 'brat_id': 'entity2', 'case_y': 'case'})\n",
    "\n",
    "    # merge entities for relationship\n",
    "    entity_relation = entity_1r.merge(entity_2r, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    cols_to_keep = ['relation_type_x', 'entity1_r_x', 'entity2_r_x', 'text_entity1', 'entity1_begin', 'entity1_end', 'entity1',\n",
    "                   'text_entity2', 'entity2_begin', 'entity2_end', 'entity2', 'case_x']\n",
    "    entity_relation = entity_relation[cols_to_keep]\n",
    "    entity_relation = entity_relation.rename(columns={'relation_type_x': 'relation_type', 'entity1_r_x': 'entity1_r', 'entity2_r_x': 'entity2_r', 'case_x': 'case'})\n",
    "\n",
    "    return entity_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from .ann df\n",
    "def get_gold_attributes(test, span_comp):\n",
    "    comp_a = test[test.attribute_entity.notnull()]\n",
    "    cols_to_keep = ['attribute_type', 'attribute_entity', 'attribute_degree', 'brat_id', 'case']\n",
    "    comp_a = comp_a[cols_to_keep]\n",
    "    entity_a = comp_a.merge(span_comp, how='left', left_on=['attribute_entity','case'], right_on=['brat_id', 'case'])\n",
    "    entity_a = entity_a.rename(columns={'case_x': 'case', 'brat_id_x': 'brat_id'})\n",
    "    cols_to_keep = ['attribute_type', 'attribute_entity', 'attribute_degree', 'brat_id', 'text']\n",
    "    entity_a = entity_a[cols_to_keep]\n",
    "   \n",
    "    return entity_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(df, ann_type):\n",
    "\n",
    "    data_out = []\n",
    "    for row in df.itertuples(index=True):\n",
    "        d = {'case': getattr(row, 'case'), \n",
    "             'begin': getattr(row, 'begin'), \n",
    "             'end': getattr(row, 'end'), \n",
    "             'text': getattr(row, 'text')}\n",
    "        if ann_type == 'system':\n",
    "            d['system'] = getattr(row, 'system')\n",
    "            d['type'] = getattr(row, 'type')\n",
    "        elif ann_type == 'gold':\n",
    "            d['gold_entity_type'] = getattr(row, 'entity_type')\n",
    "        \n",
    "        if d not in data_out:\n",
    "            data_out.append(d)\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_system_list(gold, system):\n",
    "\n",
    "    gold_out = []\n",
    "    for row in gold.itertuples(index=True):\n",
    "        if {'case': getattr(row, 'case'), 'begin': getattr(row, 'begin'), 'end': getattr(row, 'end'), 'text': getattr(row, 'text'), 'gold_entity_type': getattr(row, 'entity_type')} not in gold_out:\n",
    "            gold_out.append({'case': getattr(row, 'case'), 'begin': getattr(row, 'begin'), 'end': getattr(row, 'end'), 'text': getattr(row, 'text'), 'gold_entity_type': getattr(row, 'entity_type')})\n",
    "\n",
    "    system_out = []\n",
    "    for row in system.itertuples(index=True):\n",
    "        if {'case': getattr(row, 'case'), 'begin': getattr(row, 'begin'), 'end': getattr(row, 'end'), 'text': getattr(row, 'text'), 'system': getattr(row, 'system'), 'type': getattr(row, 'type')} not in system_out:\n",
    "            system_out.append({'case': getattr(row, 'case'), 'begin': getattr(row, 'begin'), 'end': getattr(row, 'end'), 'text': getattr(row, 'text'), 'system': getattr(row, 'system'), 'type': getattr(row, 'type')})\n",
    "\n",
    "    return gold_out, system_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(gold_out, system_out):\n",
    "    # system annotation contained in gold annotation\n",
    "    matches = []\n",
    "    \n",
    "    for g in gold_out:\n",
    "        g_begin = int(g['begin'])\n",
    "        g_end = int(g['end'])\n",
    "        g_case = g['case']\n",
    "        for s in system_out:\n",
    "            s_begin = int(s['begin'])\n",
    "            s_end = int(s['end'])\n",
    "            s_case = s['case']\n",
    "            if (((s==g) or (s_begin >= g_begin and s_end < g_end) or (s_begin > g_begin and s_end <= g_end) or \n",
    "                (g_begin >= s_begin and g_end < s_end) or (g_begin > s_begin and g_end <= s_end)) and ((s, g) not in matches) and g_case == s_case):\n",
    "                \n",
    "                    if len(s['text'])*2 >= len(g['text']): \n",
    "                    #print('gold standard anotation: {0}, biomedicus annotation {1}'.format(g, b))\n",
    "                        matches.append((s, g))\n",
    "                \n",
    "    #print(matches)\n",
    "    d = []\n",
    "    for m in matches:\n",
    "        #print({'span_g': (m[1]['begin'], m[1]['end']), 'span_b': (m[0]['begin'], m[0]['end']), 'text_g': m[1]['text'], 'text_b': m[0]['text'], 'type': m[0]['type']})\n",
    "        d.append({'case': getattr(row, 'case'), 'span_gold': (m[1]['begin'], m[1]['end']), 'span_system': (m[0]['begin'], m[0]['end']), 'text_gold': m[1]['text'], 'text_system': m[0]['text'], 'system': m[0]['system'], 'type_gold': m[1]['gold_entity_type'], 'type_system': m[0]['type']})\n",
    "\n",
    "    #print(d)\n",
    "    analysis = pd.DataFrame(d)\n",
    "    #print(analysis)\n",
    "    analysis.to_csv('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/gold_'+ system +'_summary.csv')\n",
    "    #print(matches)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             if not inner:\n",
    "#                 if (((s_begin >= g_begin and s_begin < g_end and s_end > g_end) or \n",
    "#                      (s_begin < g_begin and s_end > g_begin and s_end < g_end) or \n",
    "#                      (g_begin >= s_begin and g_begin < s_end and g_end > s_end) or \n",
    "#                      (g_begin < s_begin and g_end > s_begin and g_end < s_end)) and \n",
    "#                     ((s, g) not in c.matches) and g_case == s_case):\n",
    "#     #            if (((s_begin >= g_begin and s_begin <= g_end) or (g_begin >= s_begin and g_begin <= s_end)) and ((s, g) not in matches) and g_case == s_case):\n",
    "#                         #print('gold standard anotation: {0}, biomedicus annotation {1}'.format(g, b))\n",
    "#                     if len(s['text']) <= 2*len(g['text']): \n",
    "#                         c.matches.append((s, g))\n",
    "#                         mMatch = True\n",
    "#                         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-occurences \n",
    "def get_cooccurences(gold_out, system_out, nested = True):\n",
    "    \n",
    "    class Coocurences(object):\n",
    "        def __init__(self):\n",
    "            self.gold_system_match = 0\n",
    "            self.gold_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.gold_n = 0\n",
    "            self.matches = []\n",
    "            self.false_negatives = []\n",
    "            \n",
    "    c = Coocurences()\n",
    "    \n",
    "    #matches = []\n",
    "    #false_negatives = [] \n",
    "    #gold_only = 0\n",
    "    for g in gold_out:\n",
    "        g_begin = int(g['begin'])\n",
    "        g_end = int(g['end'])\n",
    "        g_case = g['case']\n",
    "        mMatch = False\n",
    "        for s in system_out:\n",
    "            s_begin = int(s['begin'])\n",
    "            s_end = int(s['end'])\n",
    "            s_case = s['case']\n",
    "            if nested and g_case == s_case:\n",
    "                if (((s_begin >= g_begin and s_end < g_end) or \n",
    "                     (s_begin > g_begin and s_end <= g_end) or \n",
    "                     (g_begin >= s_begin and g_end < s_end) or \n",
    "                     (g_begin > s_begin and g_end <= s_end)) and \n",
    "                    ((s, g) not in c.matches)): # and g_case == s_case):\n",
    "    #            if (((s_begin >= g_begin and s_begin <= g_end) or (g_begin >= s_begin and g_begin <= s_end)) and ((s, g) not in matches) and g_case == s_case):\n",
    "                        #print('gold standard anotation: {0}, biomedicus annotation {1}'.format(g, b))\n",
    "                    if len(s['text']) <= 2*len(g['text']): \n",
    "                        c.matches.append((s, g))\n",
    "                        mMatch = True\n",
    "                        break\n",
    "        \n",
    "            if not nested and g_case == s_case:\n",
    "                if (((s_begin >= g_begin and s_begin < g_end and s_end > g_end) or \n",
    "                     (s_begin < g_begin and s_end > g_begin and s_end < g_end) or \n",
    "                     (s_begin < g_begin and s_end > g_begin and s_end > g_end)) and # or \n",
    "                     #(g_begin < s_begin and g_end > s_begin and g_end > s_end) or \n",
    "                     #(g_begin >= s_begin and g_begin < s_end and g_end > s_end) or \n",
    "                     #(g_begin < s_begin and g_end > s_begin and g_end < s_end)) and \n",
    "                    ((s, g) not in c.matches)): # and g_case == s_case):\n",
    "    #            if (((s_begin >= g_begin and s_begin <= g_end) or (g_begin >= s_begin and g_begin <= s_end)) and ((s, g) not in matches) and g_case == s_case):\n",
    "                        #print('gold standard anotation: {0}, biomedicus annotation {1}'.format(g, b))\n",
    "                    if len(s['text']) <= 2*len(g['text']): \n",
    "                        c.matches.append((s, g))\n",
    "                        mMatch = True\n",
    "                        break\n",
    "                        \n",
    "        if mMatch == False:\n",
    "            c.gold_only += 1\n",
    "            \n",
    "            fn = {'case': None, \n",
    "                  'begin': None, \n",
    "                  'end': None, \n",
    "                  'text': None, \n",
    "                  'system': None, \n",
    "                  'type': None}\n",
    "            \n",
    "            c.false_negatives.append((fn, g))\n",
    "        \n",
    "\n",
    "#.    gives erroneous results, since not a set theoretic difference\n",
    "#     gold_test_match = len(matches)\n",
    "#     gold_only = len(gold_out) - gold_test_match\n",
    "#     test_only = len(system_out) - len(matches)\n",
    "\n",
    "    c.gold_system_match = len(c.matches)\n",
    "    c.system_only = len(system_out) - len(c.matches)\n",
    "    #gold_only = len(gold_out) - gold_test_match\n",
    "    # would use set diffrence, but cannot hash on dictionary\n",
    "    #gold_only = len([item for item in gold_out if item not in matches])\n",
    "    #system_only = len([item for item in system_out if item not in matches])\n",
    "    c.system_n = len(system_out)\n",
    "    c.gold_n = len(gold_out)\n",
    "    \n",
    "    if len(gold_out) - c.gold_system_match < 0:\n",
    "        print(c.matches)\n",
    "    \n",
    "    return c #gold_system_match, gold_only, system_only, gold_n, system_n,  matches, false_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(system_only, gold_only, gold_system_match, system_n):\n",
    "    import numpy as np\n",
    "    import math\n",
    "    from scipy import stats\n",
    "    \n",
    "    class Metrics(object):\n",
    "        \"\"\"\n",
    "        metrics \n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            self = self    \n",
    "            \n",
    "            self.system_only = system_only\n",
    "            self.gold_only = gold_only\n",
    "            self.gold_system_match = gold_system_match\n",
    "            self.system_n = system_n\n",
    "            \n",
    "            \n",
    "        def get_confusion_metrics(self):\n",
    "            \n",
    "            TP = self.gold_system_match\n",
    "            FP = self.system_only\n",
    "            FN = self.gold_only\n",
    "            TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "            confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            c = np.asarray(confusion)\n",
    "            recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "            precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if FN == 0:\n",
    "                TP_FN_R = TP\n",
    "            elif FN > 0:\n",
    "                TP_FN_R = TP/FN\n",
    "            \n",
    "            return F, recall, precision, TP, FP, FN, TP_FN_R, TM\n",
    "            \n",
    "\n",
    "    #confusion = [[0, system_only],[gold_only,gold_system_match]]\n",
    "    #c = np.asarray(confusion)\n",
    "\n",
    "#     m.TP = gold_system_match\n",
    "#     m.FP = system_only\n",
    "#     m.FN = gold_only\n",
    "#     m.TM = m.TP/math.sqrt(system_n) # TigMetric\n",
    "    \n",
    "#     if m.FN == 0:\n",
    "#         m.TP_FN_R = m.TP\n",
    "#     elif m.FN > 0:\n",
    "#         m.TP_FN_R = m.TP/m.FN\n",
    "    \n",
    "    # https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "    #m.recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "    #m.precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "    #m.F = 2*(m.precision*m.recall)/(m.precision + m.recall)\n",
    "    \n",
    "    #F, recall, precision = m.get_confusion_metrics(test_only, gold_only, gold_test_match)\n",
    "    \n",
    "    #print('F-score: {0}, precision: {1}, recall: {2}'.format(F, precision, recall))\n",
    "    \n",
    "    return Metrics() #, m.get_confusion_metrics(test_only, gold_only, gold_test_match) #precision, recall, F, FN, FP, TP, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_metrics(system_only, gold_only, gold_system_match, system_n):\n",
    "#     import numpy as np\n",
    "#     import math\n",
    "#     from scipy import stats\n",
    "    \n",
    "#     class Metrics(object):\n",
    "#         \"\"\"\n",
    "#         metrics \n",
    "#         \"\"\"\n",
    "#         def __init__(self):\n",
    "#             self = self    \n",
    "\n",
    "#             self.TN = 0\n",
    "#             self.TP = gold_system_match\n",
    "#             self.FP = 0\n",
    "#             self.FN = 0\n",
    "#             self.TM = 0\n",
    "#             self.TP_FN_R = 0\n",
    "#             self.recall = np.array([]) \n",
    "#             self.precision = np.array([])\n",
    "#             self.F = np.array([])\n",
    "            \n",
    "            \n",
    "#         def get_confusion_metrics(self, system_only, gold_only, gold_system_match):\n",
    "#             confusion = [[0, system_only],[gold_only,gold_system_match]]\n",
    "#             c = np.asarray(confusion)\n",
    "#             recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "#             precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "#             F = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "#             return F, recall, precision\n",
    "            \n",
    "#     m = Metrics()\n",
    "\n",
    "#     #confusion = [[0, system_only],[gold_only,gold_system_match]]\n",
    "#     #c = np.asarray(confusion)\n",
    "\n",
    "#     m.TP = gold_system_match\n",
    "#     m.FP = system_only\n",
    "#     m.FN = gold_only\n",
    "#     m.TM = m.TP/math.sqrt(system_n) # TigMetric\n",
    "    \n",
    "#     if m.FN == 0:\n",
    "#         m.TP_FN_R = m.TP\n",
    "#     elif m.FN > 0:\n",
    "#         m.TP_FN_R = m.TP/m.FN\n",
    "    \n",
    "#     # https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "#     #m.recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "#     #m.precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "#     #m.F = 2*(m.precision*m.recall)/(m.precision + m.recall)\n",
    "    \n",
    "#     #F, recall, precision = m.get_confusion_metrics(test_only, gold_only, gold_test_match)\n",
    "    \n",
    "#     #print('F-score: {0}, precision: {1}, recall: {2}'.format(F, precision, recall))\n",
    "    \n",
    "#     return m #, m.get_confusion_metrics(test_only, gold_only, gold_test_match) #precision, recall, F, FN, FP, TP, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall = TP/(FN + TP)\n",
    "\n",
    "#precision = TP/(FP + TP)\n",
    "#F = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "#print('F-score: {0}, precision: {1}, recall: {2}'.format(F, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_data(cs, case_config, run_all=True):\n",
    "    \n",
    "    gold_directory = cs.gold_path\n",
    "\n",
    "    cases, txt_directory, partition = case_config\n",
    "    gold_directory += txt_directory\n",
    "    \n",
    "    colnames=['brat_id', 'brat_mapping', 'text'] \n",
    "    # read in files\n",
    "    test = pd.DataFrame()\n",
    "    system_df = pd.DataFrame()\n",
    "    \n",
    "    system_out_test = [] # refactor out from gold_system_list\n",
    "    \n",
    "    if partition != 'amicus':\n",
    "        systems = cs.systems\n",
    "    else:\n",
    "        systems = cs.amicus\n",
    "    \n",
    "    i = 0\n",
    "    for case in cases:\n",
    "\n",
    "        if run_all:\n",
    "            for system in systems:\n",
    "                #system_directory = '/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/data/' + system + '_out/'\n",
    "                system_directory = cs.system_path + system + '_out/'\n",
    "                system_file = system_directory + case + '-v1.txt.xmi'\n",
    "\n",
    "                with open(system_file, 'r') as f:\n",
    "                    lines = [x.strip() for x in f.readlines()]\n",
    "\n",
    "                #lines, test, case, system = iter_cases()\n",
    "                text, xmi = system_sofa(lines)\n",
    "                #print(xmi)\n",
    "\n",
    "                df, sys_out = get_system_annotations(xmi, system, case, text)\n",
    "                system_df = pd.concat([system_df, df], ignore_index=True)\n",
    "                \n",
    "                # refactor out from gold_system_list\n",
    "                #if sys_out_dict not in system_out_test:\n",
    "                \n",
    "                system_out_test = sys_out + system_out_test\n",
    "                \n",
    "                #print('Ya Ya!', len(sys_out), type(sys_out)) \n",
    "            #print('i:', i)\n",
    "            #i += 1\n",
    "        # create dataframe of annotations from .ann \n",
    "        gold_file = gold_directory + case + '-v1.ann'\n",
    "        temp = pd.read_table(gold_file, header=None, names=colnames)\n",
    "        temp['case'] = case\n",
    "        test = pd.concat([test,temp], ignore_index=True)\n",
    "    \n",
    "    #system_df.to_csv(cs.output_path + '/system_out.csv')\n",
    "    #test.to_csv(cs.output_path + '/gold_out.csv')\n",
    "    \n",
    "    return test, system_df, system_out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(system_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get type system metrics against annotation entities\n",
    "'''\n",
    "\n",
    "def metrics_out(cs, gold_out, system_out, case_config, nested = True):\n",
    "    \n",
    "    metrics = pd.DataFrame()\n",
    "    \n",
    "    cases, txt_directory, partition = case_config\n",
    "\n",
    "    if partition != 'amicus':\n",
    "        systems = cs.systems\n",
    "    else:\n",
    "        systems = cs.amicus\n",
    "        \n",
    "    for sys in systems:\n",
    "        #print(sys)\n",
    "        for ge in cs.gold_entities:\n",
    "            gold = []\n",
    "            for g in gold_out:\n",
    "                if g['gold_entity_type'] == ge:\n",
    "                    #print(g)\n",
    "                    gold.append(g)\n",
    "            #print(ge)\n",
    "            #print(gold)\n",
    "\n",
    "            #types = get_system_type(sys)\n",
    "            types = AnnotationSystems().get_system_type(sys)\n",
    "    \n",
    "            for t in types:\n",
    "                system = [] \n",
    "                for s in system_out:\n",
    "                    if s['system'] == sys and s['type'] == t:\n",
    "                        system.append(s)\n",
    "                #print(t)\n",
    "                #print(system)\n",
    "        #             print(s['system'], s['type'], g['gold_entity_type'])\n",
    "                # TODO refactor to a class\n",
    "                #gold_system_match, gold_only, system_only, gold_n, system_n, matches, false_negatives = get_cooccurences(gold, system)\n",
    "                #gold_system_match, gold_only, system_only, gold_n, system_n, matches, false_negatives \n",
    "                c = get_cooccurences(gold, system, nested)\n",
    "\n",
    "                if c.gold_system_match > 0:\n",
    "                    #print(get_metrics(test_only, gold_only, gold_test_match))\n",
    "                    #print('{0}, {1}, {2}'.format(test_only, gold_only, gold_test_match))\n",
    "                    #precision, recall, F, FN, FP, TP, TP_FN_R, TM = get_metrics(system_only, gold_only, gold_test_match, system_n)\n",
    "                    #m = get_metrics(system_only, gold_only, gold_system_match, system_n)\n",
    "#                     F, recall, precision = m.get_confusion_metrics(system_only, gold_only, gold_system_match)\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = get_metrics(c.system_only, c.gold_only, c.gold_system_match, c.system_n).get_confusion_metrics()\n",
    "                    #data = pd.DataFrame({'system': s['system'], 'type': t, 'entity': ge, 'F': F, 'precision': precision, 'recall': recall}, )\n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'entity': ge, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.gold_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "            \n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True) # needed due to duplicates in brat entity list!! TODO: remove\n",
    "            #else:\n",
    "            #    print('no matches')\n",
    "            \n",
    "\n",
    "    #print(metrics.to_string())\n",
    "    #metrics.to_csv('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/test_metrics.csv')\n",
    "    #metrics.to_csv(cs.output_path + '/test_metrics.csv')\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all anotations\n",
    "def get_gold_ann_data():\n",
    "\n",
    "    import re, os, glob, path\n",
    "    import pandas as pd\n",
    "\n",
    "    #directory_to_parse = '/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/brat_files/all' # use for aggregate counts\n",
    "    directory_to_parse = cs.gold_path + '/all'\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    test = pd.DataFrame()\n",
    "    #for fname in glob.glob(\"/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/brat_files/all/*.ann\"):\n",
    "    for fname in glob.glob(directory_to_parse + '/*.ann'):\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0]\n",
    "\n",
    "        with open(fname) as f:\n",
    "            colnames=['brat_id', 'brat_mapping', 'text'] \n",
    "            # read in files\n",
    "            temp = pd.read_table(f.name, header=None, names=colnames)\n",
    "            temp['case'] = t.split('.')[0].split('-')[0]\n",
    "        test = pd.concat([test,temp], ignore_index=True)\n",
    "    \n",
    "    return test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output all annotations: change test variable in cell block above accordingly to get these\n",
    "# def get_all_annotations():\n",
    "#     test = get_gold_ann_data()\n",
    "\n",
    "#     test, span, comp, span_comp = get_gold_annotations(test)\n",
    "#     entity_relations = get_gold_relations(test, span_comp)\n",
    "#     entity_attributes = get_gold_attributes(test, span_comp)\n",
    "\n",
    "#     comp.to_csv('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/entity_ann_trauma.csv')\n",
    "#     entity_relations.to_csv('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/entity_relations_ann_trauma.csv')\n",
    "#     entity_attributes.to_csv('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/entity_attributes_ann_trauma.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_annotations_out(cs, comp, entity_attributes, entity_relations, partition):\n",
    "    entities = comp['entity_type'].tolist()\n",
    "    entities = set(entities)\n",
    "    #print(sorted(list(entities)))\n",
    "    \n",
    "    #print(comp)\n",
    "\n",
    "    #writer = pd.ExcelWriter('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/brat_annotated_entities.xlsx')\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_entities_'+ partition +'.xlsx')\n",
    "\n",
    "    for entity in sorted(list(entities)):\n",
    "        e = comp[comp['entity_type'] == entity]\n",
    "        e.to_excel(writer,sheet_name=entity, engine='xlsxwriter')\n",
    "\n",
    "    writer.save()\n",
    "    \n",
    "    attributes = entity_attributes['attribute_type'].tolist()\n",
    "    attributes = set(attributes)\n",
    "    #print(sorted(list(attributes)))\n",
    "\n",
    "    #writer = pd.ExcelWriter('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/brat_annotated_attributes.xlsx')\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_attributes_'+ partition +'.xlsx')\n",
    "\n",
    "    for attribute in sorted(list(attributes)):\n",
    "        a = entity_attributes[entity_attributes['attribute_type'] == attribute]\n",
    "        a.to_excel(writer,attribute)\n",
    "\n",
    "    \n",
    "    writer.save()\n",
    "    \n",
    "    relationships = entity_relations['relation_type'].tolist()\n",
    "    relationships = set(relationships)\n",
    "    #print(sorted(list(relationships)))\n",
    "\n",
    "    #writer = pd.ExcelWriter('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/brat_annotated_relationships.xlsx')\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_relationships_'+ partition + '.xlsx')\n",
    "\n",
    "    for relation in sorted(list(relationships)):\n",
    "        r = entity_relations[entity_relations['relation_type'] == relation]\n",
    "        r.to_excel(writer,relation)\n",
    "    \n",
    "    \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_attributes.groupby(['attribute_type','attribute_degree'])['attribute_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(comp.groupby(['entity_type'])['entity_type'].count())\n",
    "\n",
    "#mask = comp['entity_type'] == 'Age'\n",
    "#print(comp[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_relations.groupby(['relation_type'])['relation_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get annotation text\n",
    "'''\n",
    "def get_gold_txt(cs, case_config):\n",
    "\n",
    "    import os, glob, path\n",
    "    import pandas as pd\n",
    "\n",
    "    #directory_to_parse = '/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/txt_files/pass_one'\n",
    "\n",
    "    #os.chdir(directory_to_parse)\n",
    "\n",
    "    test = pd.DataFrame()\n",
    "    cases = []\n",
    "\n",
    "#     txt_directory = ''\n",
    "#     if partition == 'pilot':\n",
    "#         txt_directory +=  cs.pilot_directory\n",
    "#     elif partition == 'training':\n",
    "#         txt_directory += cs.training_directory\n",
    "#     elif partition == 'validation':\n",
    "#         txt_directory += cs.validation_directory\n",
    "\n",
    "    cases, txt_directory, partition = case_config\n",
    "    \n",
    "    #for fname in glob.glob(\"/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/txt_files/training_set/*.txt\"):\n",
    "    for fname in glob.glob(cs.txt_path + txt_directory + '*.txt'):\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0]\n",
    "        #print(u)\n",
    "        \n",
    "        with open(fname) as f:\n",
    "                text = f.read()\n",
    "                #print(t.split('.')[0].split('-')[0])\n",
    "                d = [text] #, 'case': t.split('.')[0].split('-')[0]}\n",
    "                \n",
    "                #print(d)\n",
    "                temp = pd.DataFrame(d, columns=['text'])\n",
    "                temp['case'] = t.split('.')[0].split('-')[0]\n",
    "                temp = temp.rename(columns={'0': 'text'})\n",
    "        cases.append(t.split('.')[0].split('-')[0])\n",
    "                #print(temp)\n",
    "        test = pd.concat([test,temp], ignore_index=True)\n",
    "    #print(test.columns)\n",
    "    \n",
    "    return test, cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create JSON with text and span as keys\n",
    "\n",
    "def annotations_to_json(span_comp, comp, gold_text):\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'entity_type']\n",
    "\n",
    "    span = span_comp.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "    span = span.rename(columns={'case_x': 'case'})\n",
    "    span = span[cols_to_keep]\n",
    "\n",
    "\n",
    "    #for case in cases:\n",
    "    mask = span['case'] == 'AC71231'\n",
    "    df = span[mask]\n",
    "\n",
    "    mask = gold_text['case'] == 'AC71231'\n",
    "    txt = gold_text[mask]\n",
    "    #print(txt['text'])\n",
    "\n",
    "    for index, row in txt.iterrows():\n",
    "        out_text = row[0]\n",
    "        out_text = re.sub('\\|',' ', out_text)\n",
    "    #def gold_ann_to_json(comp):\n",
    "\n",
    "    keyDict = {\"spans\"}\n",
    "    s = dict([(key, []) for key in keyDict])\n",
    "    #print(s)\n",
    "\n",
    "\n",
    "    s[\"text\"] = out_text\n",
    "    for index, row in df.iterrows():\n",
    "        start = row[0]\n",
    "        end = row[1]\n",
    "        label = row[3]\n",
    "        #print(row[0], row[1], row[2], row[3])\n",
    "        d = {\"start\": int(start), \"end\": int(end), \"label\": label}\n",
    "        #print(d)\n",
    "        s['spans'].append(d)\n",
    "\n",
    "    print(json.dumps([s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jsonl, csv for annotated patterns\n",
    "def annotated_patterns_to_json(cs, comp, case_config):\n",
    "    \n",
    "#     if partition == 'pilot':\n",
    "#         cases = cs.pilot_set\n",
    "#     elif partition == 'training':\n",
    "#         cases = cs.training_set\n",
    "#     elif partition == 'validation':\n",
    "#         cases = cs.validation_set\n",
    "\n",
    "    cases, txt_directory, partition = case_config\n",
    "        \n",
    "    #print(cases)\n",
    "    patterns = set()\n",
    "    for case in cases:\n",
    "\n",
    "        mask = comp['case'] == case #'AC71231'\n",
    "        mp = comp[mask]\n",
    "\n",
    "        cols_to_keep = ['text', 'entity_type']\n",
    "        mp= mp[cols_to_keep]\n",
    "\n",
    "        #f = open(\"/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/match_patterns.jsonl\", \"a\")\n",
    "        f = open(cs.output_path + '/match_patterns.jsonl', 'a')\n",
    "\n",
    "        # add to set for unique pattern\n",
    "        for index, row in mp.iterrows():\n",
    "            d = {\"text\": row[0], \"label\": row[1]} # use for seed terms\n",
    "            pattern = {\"label\": row[1], \"pattern\": row[0]}\n",
    "            patterns.add(json.dumps(pattern))\n",
    "        \n",
    "    for p in sorted(patterns):\n",
    "        #print(p)\n",
    "        f.write(p + '\\n')\n",
    "    \n",
    "    # output to spreadsheet by entity\n",
    "    cols_to_keep = ['entity_type', 'text']\n",
    "    out = comp[cols_to_keep]\n",
    "    entities = out['entity_type'].tolist()\n",
    "    entities = set(entities)\n",
    "    #print(sorted(list(entities)))\n",
    "\n",
    "    #writer = pd.ExcelWriter('/Users/gregsilverman/development/nlpie_dev/nlp/nlpie/projects/trauma/brat_annotated_entities.xlsx')\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/annotated_entities_for_patterns.xlsx')\n",
    "\n",
    "    for entity in sorted(list(entities)):\n",
    "        e = out[out['entity_type'] == entity]\n",
    "        e.drop_duplicates(keep='last', inplace=True)\n",
    "        e = e.sort_values(by=['text'])\n",
    "        e.to_excel(writer,sheet_name=entity,columns=['text'],index=False)\n",
    "        \n",
    "    \n",
    "    writer.save()\n",
    "    \n",
    "    #print(json.dumps(pattern, sort_keys=True))\n",
    "    #s.update(d)\n",
    "#f.close\n",
    "\n",
    "#print(s)\n",
    "\n",
    "# test slicing from text\n",
    "#v = myprint(text, 0, 1540)\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseSystem(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    cases by pilot, training and validation sets\n",
    "    paths by output, gold and system locations\n",
    "    directories by partition case types\n",
    "    extensions bu gold and system\n",
    "    systems \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        self.amicus = ['amicus']\n",
    "        self.pilot_directory = '/pass_one/'\n",
    "        self.new_directory = '/new_data/'\n",
    "        self.training_directory = '/training_set/'\n",
    "        self.validation_directory = '/validation_set/'\n",
    "        self.amicus_directory = '/adapt_amicus/'\n",
    "        self.all_directory = '/all/'\n",
    "        self.output_path = '/Users/gms/development/nlp/nlpie/projects/trauma/output'\n",
    "        self.gold_path = '/Users/gms/development/nlp/nlpie/projects/trauma/brat_files'\n",
    "        self.txt_path = '/Users/gms/development/nlp/nlpie/projects/trauma/txt_files'\n",
    "        self.system_path = '/Users/gms/development/nlp/nlpie/data/'\n",
    "        self.gold_ftype = '/*.ann'\n",
    "        self.system_ftype = '/*.txt'\n",
    "        \n",
    "        self.pilot_set = ['AC71231',\n",
    "                          'AC74787',\n",
    "                          'AL725222',\n",
    "                          'AL731702',\n",
    "                          'AL731704',\n",
    "                          'AL733357',\n",
    "                          'AL740639',\n",
    "                          'AL742150',\n",
    "                          'AL749501',\n",
    "                          'AL753536']\n",
    "        \n",
    "        self.training_set = ['AC71231',\n",
    "                             'AC72114',\n",
    "                             #'AC72244', # issues with too large file\n",
    "                             'AC72253',\n",
    "                             'AC72336',\n",
    "                             'AC73846',\n",
    "                             'AC73930',\n",
    "                             'AC74744',\n",
    "                             'AC74787',\n",
    "                             'AC74877',\n",
    "                             'AC7579',\n",
    "                             'AC7701',\n",
    "                             'AL711198',\n",
    "                             'AL711277',\n",
    "                             'AL711278',\n",
    "                             'AL713195',\n",
    "                             'AL714496',\n",
    "                             'AL715220',\n",
    "                             'AL715243',\n",
    "                             'AL715482',\n",
    "                             'AL719219',\n",
    "                             'AL722081',\n",
    "                             'AL722083',\n",
    "                             'AL722382',\n",
    "                             'AL724394',\n",
    "                             'AL724464',\n",
    "                             'AL725183',\n",
    "                             'AL725222',\n",
    "                             'AL725608',\n",
    "                             'AL725821',\n",
    "                             'AL730352',\n",
    "                             'AL731702',\n",
    "                             'AL731704',\n",
    "                             'AL733357',\n",
    "                             'AL734634',\n",
    "                             'AL735140',\n",
    "                             'AL735651',\n",
    "                             'AL740639',\n",
    "                             'AL741783',\n",
    "                             'AL741825',\n",
    "                             'AL741827',\n",
    "                             'AL742150',\n",
    "                             'AL742510',\n",
    "                             'AL742565',\n",
    "                             'AL742910',\n",
    "                             'AL743316',\n",
    "                             'AL743429',\n",
    "                             'AL746324',\n",
    "                             'AL749396',\n",
    "                             'AL749501',\n",
    "                             'AL753532',\n",
    "                             'AL753536',\n",
    "                             'AL76615',\n",
    "                             'AL77146',\n",
    "                             'AX7941',\n",
    "                             'MS71663',\n",
    "                             'NP71049',\n",
    "                             'RF7241',\n",
    "                             'TD70320',\n",
    "                             'al76592']\n",
    "        \n",
    "        self.validation_set = ['AC71734',\n",
    "                                 'AC72148',\n",
    "                                 'AC72242',\n",
    "                                 'AC72666',\n",
    "                                 'AC72825',\n",
    "                                 'AC72996',\n",
    "                                 'AC73178',\n",
    "                                 'AC73805',\n",
    "                                 'AC74444',\n",
    "                                 'AC74464',\n",
    "                                 'AC74831',\n",
    "                                 'AC74846',\n",
    "                                 'AL710225',\n",
    "                                 'AL710238',\n",
    "                                 'AL710582',\n",
    "                                 'AL711233',\n",
    "                                 'AL711578',\n",
    "                                 'AL712404',\n",
    "                                 'AL713181',\n",
    "                                 'AL713696',\n",
    "                                 'AL713698',\n",
    "                                 'AL721960',\n",
    "                                 'AL723667',\n",
    "                                 'AL723966',\n",
    "                                 'AL724390',\n",
    "                                 'AL724391',\n",
    "                                 'AL724913',\n",
    "                                 'AL725290',\n",
    "                                 'AL726570',\n",
    "                                 'AL728175',\n",
    "                                 'AL729303',\n",
    "                                 'AL730355',\n",
    "                                 'AL732160',\n",
    "                                 'AL732487',\n",
    "                                 'AL733571',\n",
    "                                 'AL733991',\n",
    "                                 'AL736690',\n",
    "                                 'AL737026',\n",
    "                                 'AL737342',\n",
    "                                 'AL737916',\n",
    "                                 'AL738954',\n",
    "                                 'AL738975',\n",
    "                                 'AL739435',\n",
    "                                 'AL740173',\n",
    "                                 'AL740913',\n",
    "                                 'AL743595',\n",
    "                                 'AL744275',\n",
    "                                 'AL744464',\n",
    "                                 'AL744694',\n",
    "                                 'AL745137',\n",
    "                                 'AL747275',\n",
    "                                 'AL747982',\n",
    "                                 'AL748508',\n",
    "                                 'AL748716',\n",
    "                                 'AL752098',\n",
    "                                 'AL752157',\n",
    "                                 'AL76062',\n",
    "                                 'AL77500',\n",
    "                                 'RF7718',\n",
    "                                 'TG70292',\n",
    "                                 'TW70891',\n",
    "                                 'WS71500',\n",
    "                                 'ac73876']\n",
    "\n",
    "        \n",
    "        self.new_set = ['AC71056',\n",
    "                         'AC71361',\n",
    "                         'AC71554',\n",
    "                         'AC71586',\n",
    "                         'AC71789',\n",
    "                         'AC71817',\n",
    "                         'AC71884',\n",
    "                         'AC71902',\n",
    "                         'AC72080',\n",
    "                         'AC72098',\n",
    "                         'AC72254',\n",
    "                         'AC72383',\n",
    "                         'AC72560',\n",
    "                         'AC72640',\n",
    "                         'AC72785',\n",
    "                         'AC72995',\n",
    "                         'AC73035',\n",
    "                         'AC73312',\n",
    "                         'AC73677',\n",
    "                         'AC73807',\n",
    "                         'AC73856',\n",
    "                         'AC74181',\n",
    "                         'AC74187',\n",
    "                         'AC74533',\n",
    "                         'AC74601',\n",
    "                         'AC74616',\n",
    "                         'AC7493',\n",
    "                         'AC74933',\n",
    "                         'AC75032',\n",
    "                         'AC7616',\n",
    "                         'AC7688',\n",
    "                         'AC7969',\n",
    "                         'PZ72028']\n",
    "\n",
    "        self.gold_entities = ['Age',\n",
    "                         'AirbagPresence',\n",
    "                         'DriverPassengerStatus',\n",
    "                         'EjectFromCar',\n",
    "                         'Entrapment',\n",
    "                         'Extricationtime',\n",
    "                         'Gender',\n",
    "                         'HeadOn',\n",
    "                         'IndicationProcedure', \n",
    "                         'InsuranceStatus',\n",
    "                         'LocationIntrusion',\n",
    "                         'OtherMinor',\n",
    "                         'OtherSevere',\n",
    "                         'Procedure',\n",
    "                         'Rollover',\n",
    "                         'SeatbeltPresence',\n",
    "                         'SeverityIntrusion',\n",
    "                         'TBone',\n",
    "                         'VehicleSpeed']\n",
    "        \n",
    "        self.gold_amicus_entities = ['Age',\n",
    "                         'AirbagPresence',\n",
    "                         'DriverPassengerStatus',\n",
    "                         'HeadOn',\n",
    "                         'IndicationProcedure', \n",
    "                         'InsuranceStatus',\n",
    "                         'OtherMinor',\n",
    "                         'Procedure',\n",
    "                         'SeatbeltPresence',\n",
    "                         'SeverityIntrusion',\n",
    "                         'TBone',\n",
    "                         'VehicleSpeed']\n",
    "        \n",
    "    # default config to training   \n",
    "    def case_config(self, partition='training'):\n",
    "        txt_directory = ''\n",
    "            \n",
    "        if partition == 'pilot':\n",
    "            txt_directory +=  self.pilot_directory\n",
    "            cases = self.pilot_set\n",
    "        elif partition == 'training':\n",
    "            txt_directory += self.training_directory\n",
    "            cases = self.training_set\n",
    "        elif partition == 'validation':\n",
    "            txt_directory += self.validation_directory\n",
    "            cases = self.validation_set\n",
    "        elif partition == 'all':\n",
    "            txt_directory += self.all_directory\n",
    "            cases = self.training_set + self.validation_set\n",
    "        elif partition == 'amicusall': # run against 4-system output/full set\n",
    "            txt_directory += self.amicus_directory\n",
    "            cases = list(set(self.training_set + self.validation_set) - set(self.pilot_set))\n",
    "        elif partition == 'amicus': # run against amicus munged set\n",
    "            txt_directory += self.amicus_directory\n",
    "            cases = list(set(self.training_set + self.validation_set) - set(self.pilot_set))\n",
    "            #experiment_set = self.amicus_directory\n",
    "        elif partition == 'new':\n",
    "            txt_directory += self.new_directory\n",
    "            cases = self.new_set\n",
    "            \n",
    "        return cases, txt_directory, partition\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cs = CaseSystem()\n",
    "#cases, txt_directory = cs.case_config('all')\n",
    "\n",
    "#print(len(cases))\n",
    "#print(txt_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_system_annotation(m, tp = True):\n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    for i in m:\n",
    "        #print({'case':i[0]['case'],'sys text':i[0]['text'],'sys':i[0]['system'],'sys type': i[0]['type'],\n",
    "        #       'sys span':(i[0]['begin'],i[0]['end']),'gold entity':i[1]['gold_entity_type'],'gold span':(i[1]['begin'],i[1]['end']),\n",
    "        #       'gold text':i[1]['text']})\n",
    "\n",
    "        if tp:\n",
    "            d = {'case':i[0]['case'],'sys text':i[0]['text'],'sys':i[0]['system'],'sys type': i[0]['type'],\n",
    "                 'sys begin':i[0]['begin'], 'sys end': i[0]['end'],'gold entity':i[1]['gold_entity_type'],\n",
    "                 'gold begin':i[1]['begin'], 'gold end': i[1]['end'], 'gold text':i[1]['text']}\n",
    "        else:\n",
    "            d = {'case':i[1]['case'],'sys text': 'FN','sys': None,'sys type': None,\n",
    "                 'sys begin': None, 'sys end': None,'gold entity':i[1]['gold_entity_type'],'gold begin':i[1]['begin'], \n",
    "                 'gold end': i[1]['end'], 'gold text':i[1]['text']}\n",
    "\n",
    "        #print(d)\n",
    "        \n",
    "        data = pd.DataFrame(d, index=[0])\n",
    "        \n",
    "        temp = pd.concat([temp, data], ignore_index=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_in_class():\n",
    "    best ='''biomedicus Number Age\n",
    "ctakes Sentence AirbagPresence\n",
    "ctakes FractionAnnotation DriverPassengerStatus\n",
    "biomedicus IndefiniteQuantifierCue EjectFromCar\n",
    "biomedicus OtherAcronymSense Entrapment\n",
    "ctakes Predicate HeadOn\n",
    "ctakes SignSymptomMention IndicationProcedure\n",
    "metamap Candidate InsuranceStatus\n",
    "ctakes Predicate OtherMinor\n",
    "biomedicus OtherAcronymSense OtherSevere\n",
    "ctakes RomanNumeralAnnotation Procedure\n",
    "biomedicus Acronym Procedure\n",
    "ctakes Predicate Rollover\n",
    "metamap Phrase SeatbeltPresence\n",
    "ctakes SignSymptomMention SeatbeltPresence\n",
    "biomedicus IndefiniteQuantifierCue SeverityIntrusion\n",
    "biomedicus Acronym TBone\n",
    "biomedicus IndefiniteQuantifierCue VehicleSpeed\n",
    "'''.replace('\\n',' ').split(' ')\n",
    "\n",
    "    best_of = list()\n",
    "\n",
    "    i = 0\n",
    "    for b in best:\n",
    "        if i == 0:\n",
    "            d = {'system': b}\n",
    "        elif i == 1:\n",
    "            d['type'] = b\n",
    "        elif i == 2:\n",
    "            d['entity'] = b\n",
    "        i += 1\n",
    "        if i == 3:\n",
    "            i = 0\n",
    "            best_of.append(d)\n",
    "\n",
    "    return best_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_mean(metrics, cs):\n",
    "    \n",
    "    from scipy.stats.mstats import gmean\n",
    "    # 1. Group by entity type\n",
    "    # 2. Get rank average of F1, TP/FN, TM\n",
    "    # http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "    # https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    # 3. Take geomean of 2.\n",
    "    # https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \n",
    "    test = pd.DataFrame()\n",
    "    for g in cs.gold_entities:\n",
    "        df = metrics[metrics['entity'] == g]\n",
    "\n",
    "        df['F1 rank']=df['F'].rank(ascending=0,method='average')\n",
    "        df['TP/FN rank']=df['TP/FN'].rank(ascending=0,method='average')\n",
    "        df['TM rank']=df['TM'].rank(ascending=0,method='average')\n",
    "        df['Gmean'] = gmean(df.iloc[:,-3:],axis=1)\n",
    "\n",
    "        frames = [test, df]\n",
    "        test = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('a')\n",
    "# test, system, system_out = get_evaluation_data(cs, cs.case_config('pilot'), run_all=True)\n",
    "# test, span, comp, span_comp = get_gold_annotations(test)\n",
    "# print('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Run: 1->generate data; 2->brat ann out; 3->patterns out; 4->metrics; 5->top n bic w/ TP/FN; 6->amicus mash; 7->metrics 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'system': 'clamp', 'type': 'Sentence', 'entity': 'SeverityIntrusion'}, {'system': 'ctakes', 'type': 'Sentence', 'entity': 'SeverityIntrusion'}]\n",
      "   Unnamed: 0  system      type             entity         F  precision  \\\n",
      "0         147   clamp  Sentence  SeverityIntrusion  0.006479   0.003255   \n",
      "1         150  ctakes  Sentence  SeverityIntrusion  0.002391   0.001200   \n",
      "\n",
      "     recall  TP  FN     FP     TP/FN  n_gold  n_sys        TM  F1 rank  \\\n",
      "0  0.661290  82  42  25108  1.952381     124  25190  0.516654      1.0   \n",
      "1  0.306452  38  86  31619  0.441860     124  31657  0.213574      3.0   \n",
      "\n",
      "   TP/FN rank  TM rank     Gmean  \n",
      "0         1.0      1.0  1.000000  \n",
      "1         3.0      2.0  2.620741  \n",
      "       case                                           sys text    sys  \\\n",
      "0  AL725290                                         site; > 18  clamp   \n",
      "1  AL738975                                                in.  clamp   \n",
      "2  AL738975                                                in.  clamp   \n",
      "3   TD70320                                                in.  clamp   \n",
      "4   TD70320                                                in.  clamp   \n",
      "5   AL77146                                                in.  clamp   \n",
      "6   AL77146                                                in.  clamp   \n",
      "7  AL722081                                                in.  clamp   \n",
      "8  AL722081                                                in.  clamp   \n",
      "9  AL722081  significant exterior damage and airbags deployed.  clamp   \n",
      "\n",
      "   sys type  sys begin  sys end        gold entity  gold begin  gold end  \\\n",
      "0  Sentence        991     1001  SeverityIntrusion        1002      1001   \n",
      "1  Sentence       1267     1270  SeverityIntrusion        1262      1270   \n",
      "2  Sentence       1291     1294  SeverityIntrusion        1286      1294   \n",
      "3  Sentence        967      970  SeverityIntrusion         962       970   \n",
      "4  Sentence        991      994  SeverityIntrusion         986       994   \n",
      "5  Sentence        901      904  SeverityIntrusion         896       904   \n",
      "6  Sentence        925      928  SeverityIntrusion         920       928   \n",
      "7  Sentence       1304     1307  SeverityIntrusion        1299      1307   \n",
      "8  Sentence       1328     1331  SeverityIntrusion        1323      1331   \n",
      "9  Sentence       2205     2254  SeverityIntrusion        2205      2232   \n",
      "\n",
      "                     gold text  \n",
      "0                     in. > 18  \n",
      "1                     > 12 in.  \n",
      "2                     > 18 in.  \n",
      "3                     > 12 in.  \n",
      "4                     > 18 in.  \n",
      "5                     > 12 in.  \n",
      "6                     > 18 in.  \n",
      "7                     > 12 in.  \n",
      "8                     > 18 in.  \n",
      "9  significant exterior damage  \n",
      "       case sys text     sys  sys type  sys begin  sys end        gold entity  \\\n",
      "0  AL725290     > 12  ctakes  Sentence        964      968  SeverityIntrusion   \n",
      "1  AL725290     > 18  ctakes  Sentence        997     1001  SeverityIntrusion   \n",
      "2  AL738975     > 12  ctakes  Sentence       1262     1266  SeverityIntrusion   \n",
      "3  AL738975     > 18  ctakes  Sentence       1286     1290  SeverityIntrusion   \n",
      "4   TD70320     > 12  ctakes  Sentence        962      966  SeverityIntrusion   \n",
      "5   TD70320     > 18  ctakes  Sentence        986      990  SeverityIntrusion   \n",
      "6   AL77146     > 12  ctakes  Sentence        896      900  SeverityIntrusion   \n",
      "7   AL77146     > 18  ctakes  Sentence        920      924  SeverityIntrusion   \n",
      "8  AL722081     > 12  ctakes  Sentence       1299     1303  SeverityIntrusion   \n",
      "9  AL722081     > 18  ctakes  Sentence       1323     1327  SeverityIntrusion   \n",
      "\n",
      "   gold begin  gold end gold text  \n",
      "0         964       980   > 12 in  \n",
      "1        1002      1001  in. > 18  \n",
      "2        1262      1270  > 12 in.  \n",
      "3        1286      1294  > 18 in.  \n",
      "4         962       970  > 12 in.  \n",
      "5         986       994  > 18 in.  \n",
      "6         896       904  > 12 in.  \n",
      "7         920       928  > 18 in.  \n",
      "8        1299      1307  > 12 in.  \n",
      "9        1323      1331  > 18 in.  \n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import time\n",
    "# Task: parse xmi, brat cases and do \"stuff\" with data\n",
    "def main():\n",
    "    \n",
    "    cs = CaseSystem()\n",
    "    start = time.time()\n",
    "    rtype = int(input(\"Run: 1->generate data; 2->brat ann out; 3->patterns out; 4->metrics; 5->top n bic w/ TP/FN; 6->amicus mash; 7->metrics\"))\n",
    "    \n",
    "    nested = False # used to control type of overlapping match\n",
    "    nest = 'system_coverage'\n",
    "    \n",
    "    partition = 'amicusall'\n",
    "    if (rtype == 1):\n",
    "        # option 1: run evaluation on ADAPT versus brat\n",
    "        #partition = 'amicusall'\n",
    "        \n",
    "        test, system, system_out = get_evaluation_data(cs, cs.case_config(partition), run_all=True)\n",
    "        \n",
    "        # write to output to save time!\n",
    "        system.to_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        \n",
    "        print('end get eval data', (time.time() - start))\n",
    "        test, span, comp, span_comp = get_gold_annotations(test)\n",
    "        \n",
    "        print('end get gold annotations', (time.time() - start))\n",
    "\n",
    "        gold = span.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "        gold.to_csv(cs.output_path + '/gold_out'+ partition + '.csv')\n",
    "        \n",
    "        cols_to_keep = ['case', 'begin', 'end', 'text', 'entity_type']\n",
    "        \n",
    "        #gold = gold[cols_to_keep]\n",
    "        #gold = gold.rename(columns={'entity_type': 'gold_entity_type'})\n",
    "        #print(gold.to_dict('records'))\n",
    "        #gold_out = gold.to_dict('records')\n",
    "        gold_out = df_to_list(gold, 'gold')\n",
    "        \n",
    "        #m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), False) \n",
    "        # 'nested' sets the type of span coverage: True: gold covers system; False: system covers gold\n",
    "      \n",
    "        m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), nested)\n",
    "        \n",
    "        print('end metrics out', (time.time() - start))\n",
    "\n",
    "        print(m.head())\n",
    "        # get geometric mean of ranked averages of F1, TP/FN, TM\n",
    "        metrics = geometric_mean(m, cs)\n",
    "        \n",
    "        print('end geometric mean', (time.time() - start))\n",
    "        metrics.to_csv(cs.output_path + '/test_metrics'+ partition + nest + '.csv')\n",
    "        \n",
    "        print(gold.head())\n",
    "        print(system.head())\n",
    "        # write to output to save time!\n",
    "        #system.to_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        #gold.to_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "    elif (rtype == 2):\n",
    "        # option 2: write brat annotations to Excel\n",
    "        #partition = 'new'\n",
    "        test, system, system_out = get_evaluation_data(cs, cs.case_config(partition), run_all=False)\n",
    "        test, span, comp, span_comp = get_gold_annotations(test)\n",
    "        entity_relations = get_gold_relations(test, span_comp)\n",
    "        entity_attributes = get_gold_attributes(test, span_comp)\n",
    "        all_annotations_out(cs, comp, entity_attributes, entity_relations, partition)\n",
    "    elif (rtype == 3):\n",
    "        # option 3: get annotated JSON, matching patterns -> NER label\n",
    "        test, system, system_out = get_evaluation_data(cs, cs.case_config('training'), run_all=False)\n",
    "        test, span, comp, span_comp = get_gold_annotations(test)\n",
    "        gold_text, cases = get_gold_txt(cs, cs.case_config('training'))   \n",
    "        annotations_to_json(span_comp, comp, gold_text)\n",
    "        annotated_patterns_to_json(cs, comp, cs.case_config('training'))\n",
    "#     elif (rtype == 4):\n",
    "#         # get system -> brat annotations\n",
    "#         sys = pd.read_csv(cs.output_path + '/system_out.csv')\n",
    "#         ann = pd.read_csv(cs.output_path + '/gold_out.csv')\n",
    "        \n",
    "#         # loop through annotations by \"best-in-class\" and write output to worksheets by annotation type\n",
    "#         best = best_in_class()\n",
    "       \n",
    "#         test = pd.DataFrame()\n",
    "#         for b in best:\n",
    "#             #print(b)\n",
    "#             gold = ann[ann['entity_type'] == b['entity']]\n",
    "#             system = sys[(sys['system'] == b['system']) & (sys['type'] == b['type'])]\n",
    "        \n",
    "#             g1 = gold_list(gold)\n",
    "#             s1 = system_list(system)\n",
    "#             c = get_cooccurences(g1, s1)\n",
    "\n",
    "#             temp = gold_system_annotation(c.matches)\n",
    "#             testing = gold_system_annotation(c.false_negatives, False)\n",
    "            \n",
    "#             frames = [temp, testing]\n",
    "#             out = pd.concat(frames, ignore_index=True)\n",
    "#             frames = [test, out]\n",
    "#             test = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "#         print(test)\n",
    "        \n",
    "#         test.to_csv(cs.output_path + '/patterns_all.csv')\n",
    "    \n",
    "    elif (rtype == 7):\n",
    "        \n",
    "        partition = 'amicus'\n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        gold_out = df_to_list(ann, 'gold')\n",
    "        system_out = df_to_list(sys, 'system')\n",
    "        \n",
    "        #m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), False) # nested sets type of span coverage\n",
    "        m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), True)\n",
    "\n",
    "        print(m.head())\n",
    "        # get geometric mean of ranked averages of F1, TP/FN, TM\n",
    "        metrics = geometric_mean(m, cs)\n",
    "        print(metrics.head())\n",
    "        #metrics.to_csv(cs.output_path + '/test_metrics'+ partition +'.csv')\n",
    "        \n",
    "        \n",
    "        #data = pd.read_csv(cs.output_path + '/test_metrics.csv')\n",
    "        \n",
    "        #geometric_mean(data, cs)\n",
    "        \n",
    "        #geometric_mean(data, cs).to_csv(cs.output_path + '/bic_ranking.csv')\n",
    "    \n",
    "    elif (rtype == 4):\n",
    "        \n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics.csv')\n",
    "        \n",
    "        geometric_mean(data, cs)\n",
    "        \n",
    "        geometric_mean(data, cs).to_csv(cs.output_path + '/bic_ranking.csv')\n",
    "    \n",
    "    elif (rtype == 5):\n",
    "        \n",
    "        #partition = 'amicusall'\n",
    "        n = input('Select top n:')\n",
    "            \n",
    "        print('Processing top' + n + ' for best system types:')\n",
    "\n",
    "        # get system -> brat annotations\n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        best_in_class = []\n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics'+ partition + nest +'.csv')\n",
    "        \n",
    "        temp = pd.DataFrame()\n",
    "        for g in cs.gold_entities:\n",
    "            test = data[data['entity'] == g]\n",
    "            a = test.sort_values(by=['Gmean']).head(int(n))\n",
    "            cols_to_keep = ['system', 'type', 'entity']\n",
    "            top_n = a[cols_to_keep]\n",
    "            for index, row in top_n.iterrows():\n",
    "                best_in_class.append({'system':row['system'],'type': row['type'], 'entity': row['entity']})\n",
    "                \n",
    "            frames = [temp, a]\n",
    "            temp = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "        #temp = temp.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "        \n",
    "        #print(temp)\n",
    "        \n",
    "        writer = pd.ExcelWriter(cs.output_path + '/best_in_class_'+ partition +'_draft.xlsx')\n",
    "\n",
    "        temp.to_excel(writer,sheet_name='Top ' + n + ' sys annotations')\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        for b in best_in_class:\n",
    "            gold = ann[ann['entity_type'] == b['entity']]\n",
    "            system = sys[(sys['system'] == b['system']) & (sys['type'] == b['type'])]\n",
    "            \n",
    "            g1 = df_to_list(gold, 'gold')\n",
    "            s1 = df_to_list(system, 'system')\n",
    "            #c = get_cooccurences(g1, s1, False)\n",
    "            c = get_cooccurences(g1, s1)\n",
    "\n",
    "            temp = gold_system_annotation(c.matches)\n",
    "            \n",
    "            #print(b)\n",
    "            #print(temp)\n",
    "            testing = gold_system_annotation(c.false_negatives, False)\n",
    "            \n",
    "            #print(testing)\n",
    "            \n",
    "            frames = [temp, testing]\n",
    "            out = pd.concat(frames, ignore_index=True)\n",
    "            frames = [test, out]\n",
    "            test = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "        test.to_csv(cs.output_path + '/best_in_class_annotations_top_' + n + '_' + partition +'.csv')\n",
    "\n",
    "        for ge in cs.gold_entities:\n",
    "            t = test[test['gold entity'] == ge]\n",
    "            if not t.empty:\n",
    "                t.to_excel(writer,sheet_name=ge)\n",
    "            \n",
    "        writer.save()\n",
    "    \n",
    "    elif (rtype == 6):\n",
    "        \n",
    "        #n = input('Select top n:')\n",
    "            \n",
    "        #print('Processing top' + n + ' for best system types:')\n",
    "\n",
    "        # get system -> brat annotations\n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        entity_of_interest =  ['SeverityIntrusion'] #['IndicationProcedure']\n",
    "        best_in_class = []\n",
    "        \n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics'+ partition + nest +'.csv')\n",
    "        \n",
    "        temp = pd.DataFrame()\n",
    "        for g in entity_of_interest:\n",
    "#             test = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'IndefiniteQuantifierCue') |  \n",
    "#                          (data['type'] == 'StandaloneQuantifier') |\n",
    "#                          (data['type'] == 'Number'))]\n",
    "#             test = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'SignSymptomMention') |  \n",
    "#                          (data['type'] == 'UmlsConcept'))]\n",
    "#             test = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'Sentence') |  \n",
    "#                          (data['type'] == 'Phrase'))]\n",
    "            #a = test.sort_values(by=['Gmean']).head(int(n))\n",
    "            test = data[(data['entity'] == g) & \n",
    "                        (data['type'] == 'Sentence')]\n",
    "            cols_to_keep = ['system', 'type', 'entity']\n",
    "            top = test[cols_to_keep]\n",
    "            for index, row in top.iterrows():\n",
    "                best_in_class.append({'system':row['system'],'type': row['type'], 'entity': row['entity']})\n",
    "                \n",
    "            frames = [temp, test]\n",
    "            temp = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "            print(best_in_class)\n",
    "        \n",
    "        #temp = temp.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "        \n",
    "        print(temp)\n",
    "        \n",
    "        writer = pd.ExcelWriter(cs.output_path + '/best_in_class_all_'+ partition + ' ' + entity_of_interest[0] + nest +'.xlsx')\n",
    "\n",
    "        temp.to_excel(writer,sheet_name='All sys annotations', engine='openpyxl')\n",
    "        \n",
    "        # only choose those from pilot eval\n",
    "        #chosen_best = ['IndefiniteQuantifierCue','StandaloneQuantifier','Number'] #SeverityIntrusion -> gold covefage\n",
    "        #chosen_best = ['SignSymptomMention','UmlsConcept'] # IndicationProcedure -> gold coverage\n",
    "        #chosen_best = ['Sentence','Phrase'] # IndicationProcedure -> sys coverage\n",
    "        chosen_best = ['Sentence'] # SeverityIntrusion -> sys coverage\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        for b in best_in_class:\n",
    "            #print(b)\n",
    "            if b['type'] in chosen_best:\n",
    "                gold = ann[ann['entity_type'] == b['entity']]\n",
    "                system = sys[(sys['system'] == b['system']) & (sys['type'] == b['type'])]\n",
    "\n",
    "                g1 = df_to_list(gold, 'gold')\n",
    "                s1 = df_to_list(system, 'system')\n",
    "                c = get_cooccurences(g1, s1)\n",
    "\n",
    "                temp = gold_system_annotation(c.matches)\n",
    "                \n",
    "                print(temp.head(10))\n",
    "\n",
    "                testing = gold_system_annotation(c.false_negatives, False)\n",
    "\n",
    "                #print('testing', testing)\n",
    "\n",
    "                frames = [temp, testing]\n",
    "                out = pd.concat(frames, ignore_index=True)\n",
    "                frames = [test, out]\n",
    "                test = pd.concat(frames, ignore_index=True)\n",
    "                \n",
    "                #print(test.head())\n",
    "            \n",
    "        test.to_csv(cs.output_path + '/best_in_class_annotations_' + partition +'.csv')\n",
    "\n",
    "        for ge in entity_of_interest:\n",
    "            t = test[test['gold entity'] == ge]\n",
    "            if not t.empty:\n",
    "                t.to_excel(writer,sheet_name=ge)\n",
    "            \n",
    "        writer.save()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "best ='''biomedicus Number Age\n",
    "ctakes Sentence AirbagPresence\n",
    "ctakes FractionAnnotation DriverPassengerStatus\n",
    "biomedicus IndefiniteQuantifierCue EjectFromCar\n",
    "biomedicus OtherAcronymSense Entrapment\n",
    "ctakes Predicate HeadOn\n",
    "ctakes SignSymptomMention IndicationProcedure\n",
    "metamap Candidate InsuranceStatus\n",
    "ctakes Predicate OtherMinor\n",
    "biomedicus OtherAcronymSense OtherSevere\n",
    "ctakes RomanNumeralAnnotation Procedure\n",
    "ctakes Predicate Rollover\n",
    "metamap Phrase SeatbeltPresence\n",
    "ctakes SignSymptomMention SeatbeltPresence\n",
    "biomedicus IndefiniteQuantifierCue SeverityIntrusion\n",
    "biomedicus Acronym TBone\n",
    "biomedicus IndefiniteQuantifierCue VehicleSpeed\n",
    "'''.replace('\\n',' ').split(' ')\n",
    "\n",
    "best_of = list()\n",
    "\n",
    "#print(best)\n",
    "\n",
    "i = 0\n",
    "for b in best:\n",
    "    if i == 0:\n",
    "        d = {'system': b}\n",
    "    elif i == 1:\n",
    "        d['type'] = b\n",
    "    elif i == 2:\n",
    "        d['entity'] = b\n",
    "    i += 1\n",
    "    if i == 3:\n",
    "        i = 0\n",
    "        best_of.append(d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = CaseSystem()\n",
    "\n",
    "c.pilot_set\n",
    "c.training_set\n",
    "c.validation_set\n",
    "\n",
    "len(c.validation_set)\n",
    "len(c.pilot_set)\n",
    "print(len(c.training_set))\n",
    "\n",
    "len(set(c.validation_set).union(set(c.training_set)))\n",
    "len(set(c.validation_set).union(set(c.training_set)))\n",
    "len(set(c.pilot_set).intersection(set(c.training_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([line.strip() for line in open(\"/Users/gms/development/nlp/nlpie/projects/trauma/brat_files/new_data/out.txt\", 'r')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
