{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent agreement: 93.33333333333333\n",
      "Cohen's kappa for all items combined: 0.8659038083318433\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "\n",
    "\n",
    "# combine all document items into list of lists\n",
    "def mk_list(_items):\n",
    "    # use only these columns for computations\n",
    "    cols = ['al711233', 'al722382', 'al734634', 'al713696', 'al733357']\n",
    "    items = []\n",
    "    for c in cols:\n",
    "        items.append(_items[c].values.tolist())\n",
    "    return items  \n",
    "\n",
    "# flatten list of lists (needed for computation of Cohen's kappa)\n",
    "def flatten(list_in):\n",
    "    flat_list = []\n",
    "    for sublist in list_in:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "    return flat_list\n",
    "\n",
    "# Get Cohen's kappa and percent agreement\n",
    "def irr(r1, r2):\n",
    "    \n",
    "    # verify row order between input files\n",
    "    r1_ae = r1['annotation elements (34)']\n",
    "    r2_ae = r2['annotation elements (34)']\n",
    "    mm = pd.concat([r1_ae, r2_ae], axis=1)\n",
    "    mm['mismatch'] = np.where((r1_ae != r2_ae), 1, 0)\n",
    "    #print(mm[mm.mismatch == 1])\n",
    "    if (mm[mm['mismatch'] != 1].empty):\n",
    "        print('Mismatch on row annotations')\n",
    "        exit \n",
    "\n",
    "    r1_items = mk_list(r1)\n",
    "    r2_items = mk_list(r2)\n",
    "    \n",
    "    # verify document lengths\n",
    "    if(len(flatten(r1_items)) == len(flatten(r2_items))):\n",
    "        n_items = len(flatten(r2_items))\n",
    "\n",
    "        # compute number of matched items\n",
    "        n = 0\n",
    "        for i in range(n_items):\n",
    "            if (flatten(r1_items)[i] == flatten(r2_items)[i]):\n",
    "                n += 1\n",
    "\n",
    "        kappa = cohen_kappa_score(flatten(r1_items), flatten(r2_items))\n",
    "\n",
    "        #print(f'Total matches across all documents: {n}')\n",
    "        #print(f'Total number of annotated items: {n_items}')\n",
    "        \n",
    "        return ((n/n_items)*100, kappa)\n",
    "    else:\n",
    "        print('Invalid number of annotations!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # read in EL's annotations\n",
    "    r1 = pd.read_excel('/Users/gms/Downloads/Trauma_IRR_EL_2.xlsx')\n",
    "    r1 = r1[['annotation elements (34)', 'al711233', 'al722382', 'al734634', 'al713696', 'al733357']]\n",
    "    # read in GR's annotations\n",
    "    r2 = pd.read_excel('/Users/gms/Downloads/Trauma_IRR_GR_2.xlsx')\n",
    "    r2 = r2[['annotation elements (34)', 'al711233', 'al722382', 'al734634', 'al713696', 'al733357']]     \n",
    "\n",
    "    pa, k = irr(r1, r2)\n",
    "    print(f'Percent agreement: {pa}')\n",
    "    print(f'Cohen\\'s kappa for all items combined: {k}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
