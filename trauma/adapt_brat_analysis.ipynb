{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupterLab Notebook to support NLP-ADAPT experiment\n",
    "## Copyright (c) Greg M. Silverman: Regents of the University of Minnesota\n",
    "\n",
    "**Data prerequisites for analysis:**\n",
    "1. Output from n-annotator systems (see [NLP-ADAPT](https://github.com/nlpie/nlp-adapt))\n",
    "2. Manually annotated documents using BRAT annotation software\n",
    "3. word2phrase trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmljson import badgerfish as bf\n",
    "from xmljson import cobra as ca \n",
    "from xml.etree.ElementTree import fromstring\n",
    "import json\n",
    "from json import dumps\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "import os, glob, path\n",
    "from scipy.stats.mstats import gmean\n",
    "import time\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process CAS XMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/douglasmiranda/5127251\n",
    "def find(key, dictionary):\n",
    "    \"\"\"\n",
    "    find key by given value in nested JSON\n",
    "    \"\"\"\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            for result in find(key, v):\n",
    "                yield result\n",
    "        elif isinstance(v, list):\n",
    "            for d in v:\n",
    "                for result in find(key, d):\n",
    "                    yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_sofa(lines):   \n",
    "    \"\"\"\n",
    "    travese nested JSON and extract text block for subject of analysis  \n",
    "    \"\"\"\n",
    "    \n",
    "    data = dumps(ca.data(fromstring(lines[0])))\n",
    "    d = json.loads(data)\n",
    "\n",
    "    # print to get JSON representation of file\n",
    "    # print(d)\n",
    "\n",
    "    xmi = d.get(\"{http://www.omg.org/XMI}XMI\").get(\"children\")\n",
    "    for x in xmi:\n",
    "        if x.get('{http:///uima/cas.ecore}Sofa'):\n",
    "            is_sofa = x.get('{http:///uima/cas.ecore}Sofa')\n",
    "            if list(find('sofaString', is_sofa)):\n",
    "                text = is_sofa \n",
    "    return text, xmi\n",
    "\n",
    "def myprint(d, begin, end):\n",
    "    \"\"\"\n",
    "    maps span to associated text\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    out = ''\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            myprint(v, begin, end)\n",
    "            return myprint(v, begin, end)\n",
    "        elif i == 3 and k != 'mimeType':\n",
    "            if v:\n",
    "                return v[begin:end]\n",
    "        i += 1\n",
    "\n",
    "# test\n",
    "# test slicing from text\n",
    "#v = myprint(text, 0, 1540)\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization class for UIMA system annotation retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationSystems(object):\n",
    "    \"\"\"\n",
    "    CAS XMI -> JSON mappings done as per Cobra convbention\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"\n",
    "        self.annotation_type_system = ['http:///org/metamap/uima/ts.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/syntax.ecore',\n",
    "                                     'http:///org/apache/uima/ruta/type.ecore',\n",
    "                                     'http:///edu/uth/clamp/nlp/typesystem.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/textsem.ecore',\n",
    "                                     'http:///org/apache/ctakes/typesystem/type/textspan.ecore',\n",
    "                                     'http:///biomedicus/v2.ecore']\n",
    "        \"\"\"\n",
    "        annotaion relation types: TODO -> future examination of these\n",
    "        \"\"\"\n",
    "        self.annotation_type_relations = ['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA',\n",
    "                                     '{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument',\n",
    "                                     '{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']\n",
    "\n",
    "        self.annotation_relation_attributes = ['ClampRelationUIMA', 'SemanticRoleRelation']\n",
    "\n",
    "        \"\"\"\n",
    "        system types of ineterest\n",
    "        \"\"\"\n",
    "        self.biomedicus_types = ['Acronym',\n",
    "                         'DayOfWeek',\n",
    "                         'DictionaryTerm',\n",
    "                         'Fuzzy',\n",
    "                         'Historical',\n",
    "                         'IndefiniteQuantifierCue',\n",
    "                         'ModificationCue', \n",
    "                         #'Negated',\n",
    "                         'NormForm', \n",
    "                         'Number',\n",
    "                         'NumberRange',\n",
    "                         'OtherAcronymSense',\n",
    "                         'ParseToken',\n",
    "                         'SeasonWord', \n",
    "                         'StandaloneQuantifier',\n",
    "                         'TemporalPhrase',\n",
    "                         'TextData', \n",
    "                         'TimeUnits',\n",
    "                         'UmlsConcept',\n",
    "                         'YearNumber']\n",
    "        \n",
    "        self.clamp_types = ['All',\n",
    "                             'Any', \n",
    "                             'BaseToken',\n",
    "                             #'CW',\n",
    "                             'Chunk',\n",
    "                             'ClampNameEntityUIMA',\n",
    "                             #'ClampRelationUIMA', # TODO with NN\n",
    "                             #'ConllDependencyNode',\n",
    "                             'NUM',\n",
    "                             'RutaBasic',\n",
    "                             #'SPACE',\n",
    "                             #'SPECIAL',\n",
    "                             #'SW',\n",
    "                             'Segment',\n",
    "                             'Sentence',\n",
    "                             'TokenSeed',\n",
    "                             'W']\n",
    "        \n",
    "        self.ctakes_types = ['AnatomicalSiteMention',\n",
    "                             'ConllDependencyNode',\n",
    "                             'ContractionToken',\n",
    "                             'DateAnnotation',\n",
    "                             'DiseaseDisorderMention',\n",
    "                             'EntityMention',\n",
    "                             'EventMention',\n",
    "                             'FractionAnnotation',\n",
    "                             'IdentifiedAnnotation',\n",
    "                             'MeasurementAnnotation',\n",
    "                             'MedicationMention',\n",
    "                             'NumeToken',\n",
    "                             'Predicate',\n",
    "                             'ProcedureMention',\n",
    "                             'RangeAnnotation',\n",
    "                             'RomanNumeralAnnotation',\n",
    "                             'SemanticArgument', # TODO with NN\n",
    "                             #'SemanticRoleRelation',\n",
    "                             'Sentence',\n",
    "                             'SignSymptomMention',\n",
    "                             'UmlsConcept',\n",
    "                             'WordToken']\n",
    "        \n",
    "        self.metamap_types = ['AcronymAbbrev',\n",
    "                             'Annotation',\n",
    "                             'AnnotationBase',\n",
    "                             'Candidate',\n",
    "                             #'Negation'#,\n",
    "                             'Phrase',\n",
    "                             'Span',\n",
    "                             'Utterance']\n",
    "        \n",
    "        self.amicus_types = ['AnatomicalSiteMention',\n",
    "                             'Candidate',\n",
    "                             'Chunk',\n",
    "                             'IndefiniteQuantifierCue',\n",
    "                             #'MedicationMention',\n",
    "                             'Number',\n",
    "                             'Phrase',\n",
    "                             'Predicate',\n",
    "                             'SemanticArgument',\n",
    "                             'SignSymptomMention',\n",
    "                             'StandaloneQuantifier',\n",
    "                             'UmlsConcept']\n",
    "        \n",
    "        self.amicus_type = ['IndefiniteQuantifierCue','StandaloneQuantifier','Number']\n",
    "        \n",
    "        \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == 'amicus':\n",
    "            types = self.amicus_types\n",
    "\n",
    "        return types\n",
    "\n",
    "\n",
    "def get_system_annotations(xmi, system, case, text, type_of_analysis=None): \n",
    "    \n",
    "    \"\"\"\n",
    "    traverse JSON representation of XMI CAS object as represented by nested JSON object mapping as per Cobra convention\n",
    "    NB: use type_of_analysis to control flow for relationship annotations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    annSys = AnnotationSystems()\n",
    "    types = annSys.get_system_type(system)\n",
    "    \n",
    "    annotation_out = []\n",
    "    \n",
    "    # traverse xmi JSON blob\n",
    "    for x in xmi:\n",
    "        for t in types: # parse by system type\n",
    "            for ats in annSys.annotation_type_system: # with base types\n",
    "\n",
    "                if list(find('{' + ats + '}' + t, x)):\n",
    "                    if type_of_analysis is not None and t in annSys.annotation_relation_attributes: # traverse relationship type\n",
    "                        # ['ClampRelationUIMA', 'SemanticRoleRelation']:\n",
    "                        if t == 'ClampRelationUIMA':\n",
    "                             #'Relation', need to resolve linkage by semantic argument and predicate ids\n",
    "                             # link each by 'relation' \n",
    "                             # <textsem:SemanticRoleRelation argument=\"34379\" category=\"A1\" conditional=\"false\" confidence=\"0.0\" discoveryTechnique=\"0\" id=\"0\" polarity=\"0\" predicate=\"34372\" uncertainty=\"0\" xmi:id=\"34385\"/>\n",
    "                             # <textsem:SemanticArgument begin=\"1400\" end=\"1403\" label=\"A1\" relation=\"34385\" sofa=\"1\" xmi:id=\"34379\"/>\n",
    "                             # <textsem:Predicate begin=\"1385\" end=\"1396\" frameSet=\"extricate.01\" relations=\"34385\" sofa=\"1\" xmi:id=\"34372\"/>\n",
    "                            #print(t['{' + ats + '}' + token]['attributes']['entTo'])\n",
    "                            #print(t['{' + ats + '}' + token]['attributes']['entFrom'])\n",
    "\n",
    "                            endTo = x['{' + ats + '}' + t]['attributes']['entTo']\n",
    "                            endFrom = x['{' + ats + '}' + t]['attributes']['entFrom']\n",
    "\n",
    "                            #print('{0}, {1}'.format('PARENT',  t['{' + ats + '}' + token]['attributes']))\n",
    "                            for x1 in xmi:\n",
    "                                if list(find('{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA', x1)):\n",
    "                                    if x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['{http://www.omg.org/XMI}id'] == endTo:\n",
    "                                        #print(x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endTo', endTo, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                        x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endTo', endTo, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                    if x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['{http://www.omg.org/XMI}id'] == endFrom:\n",
    "                                        #print(x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endFrom', endFrom, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                                        x1['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('endFrom', endFrom, x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['begin'],x['{http:///edu/uth/clamp/nlp/typesystem.ecore}ClampNameEntityUIMA']['attributes']['end']))\n",
    "                        if t == 'SemanticRoleRelation':\n",
    "                             #'Relation', need to resolve linkage by semantic argument and predicate ids\n",
    "                             # link each by 'relation' \n",
    "                             # <textsem:SemanticRoleRelation argument=\"34379\" category=\"A1\" conditional=\"false\" confidence=\"0.0\" discoveryTechnique=\"0\" id=\"0\" polarity=\"0\" predicate=\"34372\" uncertainty=\"0\" xmi:id=\"34385\"/>\n",
    "                             # <textsem:SemanticArgument begin=\"1400\" end=\"1403\" label=\"A1\" relation=\"34385\" sofa=\"1\" xmi:id=\"34379\"/>\n",
    "                             # <textsem:Predicate begin=\"1385\" end=\"1396\" frameSet=\"extricate.01\" relations=\"34385\" sofa=\"1\" xmi:id=\"34372\"/>\n",
    "#                             print(t['{' + ats + '}' + token]['attributes']['argument'])\n",
    "#                             print(t['{' + ats + '}' + token]['attributes']['predicate'])\n",
    "#                             print(t['{' + ats + '}' + token]['attributes'])\n",
    "\n",
    "                            argument = t['{' + ats + '}' + t]['attributes']['argument']\n",
    "                            predicate = t['{' + ats + '}' + t]['attributes']['predicate']\n",
    "\n",
    "                            for x1 in xmi:\n",
    "                                if list(find('{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument', x1)):\n",
    "                                    if x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['{http://www.omg.org/XMI}id'] == argument:\n",
    "                                        #print(x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', argument, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['end']))\n",
    "                                        x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', argument, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}SemanticArgument']['attributes']['end']))\n",
    "                            for x1 in xmi:\n",
    "                                 if list(find('{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate', x)):\n",
    "                                    if x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['{http://www.omg.org/XMI}id'] == predicate:\n",
    "                                        #print(x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes'])\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', predicate, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['end']))\n",
    "                                        x1['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']\n",
    "                                        #print('{0}, {1}, {2}, {3}'.format('argument', predicate, x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['begin'],x['{http:///org/apache/ctakes/typesystem/type/textsem.ecore}Predicate']['attributes']['end']))\n",
    "                    \n",
    "                    elif t not in annSys.annotation_relation_attributes: # normal sysetem type\n",
    "                        #if (x.get('{http:///org/metamap/uima/ts.ecore}Phrase')):\n",
    "#                         if (x.get('{http:///biomedicus/v2.ecore}UmlsConcept') or x.get('{http:///biomedicus/v2.ecore}StandaloneQuantifier')):\n",
    "#                             print('!!!!')\n",
    "#                             print(x.get('{http:///biomedicus/v2.ecore}UmlsConcept'), x.get('{http:///biomedicus/v2.ecore}StandaloneQuantifier'))\n",
    "#                             print(x['{' + ats + '}' + t]['attributes'])\n",
    "#                             print('!!!!')\n",
    "                                                                                           \n",
    "                        begin = x['{' + ats + '}' + t]['attributes']['begin']\n",
    "                        end = x['{' + ats + '}' + t]['attributes']['end']\n",
    "                        d = {'system': system, 'type': t, 'begin': begin, 'end': end, 'text': myprint(text, int(begin), int(end)), 'case': case}\n",
    "                        \n",
    "                        if d not in annotation_out:\n",
    "                            annotation_out.append(d)\n",
    "\n",
    "    df = pd.DataFrame(annotation_out)\n",
    "    \n",
    "    # test:\n",
    "    # examine output from parsed xmi\n",
    "    # print(df)\n",
    "    return df, annotation_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various methods for parsing BRAT ann files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brat\n",
    "# https://stackoverflow.com/questions/15325182/how-to-filter-rows-in-pandas-by-regex\n",
    "\n",
    "# entity = test[test.brat_id.str.startswith(\"T\")]\n",
    "# relation = test[test.brat_id.str.startswith(\"R\")]\n",
    "# attribute = test[test.brat_id.str.startswith(\"A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(string):\n",
    "    pattern = re.compile(r'\\s(\\d+).*?(\\d+$)')\n",
    "    match = re.search(pattern, string)\n",
    "    return match.groups()\n",
    "\n",
    "def get_entity_type(string):\n",
    "    return string.split()[0]\n",
    "\n",
    "def get_relation_type(string):\n",
    "    return string.split()[0]\n",
    "\n",
    "def get_relation_entities(string):\n",
    "    out = string.split()[1:3]\n",
    "    return (out[0].split(':')[1], out[1].split(':')[1])\n",
    "    \n",
    "def get_attribute_type(string):\n",
    "\n",
    "    return string.split()[0]\n",
    "\n",
    "def get_attribute_entity(string):\n",
    "    out = string.split()[1:2]\n",
    "    return out[0]\n",
    "\n",
    "def get_attribute_degree(string):\n",
    "    if len(string.split()) > 2:\n",
    "        out = string.split()[2:3]\n",
    "        return out[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for parsing BRAT annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_annotations(df):\n",
    "    \"\"\"\n",
    "    parse BRAT annotations into requisite categories\n",
    "    parse span\n",
    "    \"\"\"\n",
    "    \n",
    "    # get type of snnotation as per spec: http://brat.nlplab.org/standoff.html\n",
    "    entity = df[df.brat_id.str.startswith(\"T\")]\n",
    "    relation = df[df.brat_id.str.startswith(\"R\")]\n",
    "    attribute = df[df.brat_id.str.startswith(\"A\")]\n",
    "    \n",
    "    # segregate by entity, relation, annotation\n",
    "    df['entity_span'] = entity['brat_mapping'].apply(get_span)\n",
    "    df['entity_type'] = entity['brat_mapping'].apply(get_entity_type)\n",
    "\n",
    "    df['relation_type'] = relation['brat_mapping'].apply(get_relation_type)\n",
    "    df['relation_entities'] = relation['brat_mapping'].apply(get_relation_entities)\n",
    "\n",
    "    df['attribute_type'] = attribute['brat_mapping'].apply(get_attribute_type)\n",
    "    df['attribute_entity'] = attribute['brat_mapping'].apply(get_attribute_entity)\n",
    "    df['attribute_degree'] = attribute['brat_mapping'].apply(get_attribute_degree)\n",
    "\n",
    "    # get non-null entities \n",
    "    comp = df[df.entity_span.notnull()]\n",
    "    cols_to_keep = ['text', 'entity_span', 'brat_id', 'entity_type', 'case']\n",
    "    comp = comp[cols_to_keep]\n",
    "    \n",
    "    # split span into two separate columns\n",
    "    span = comp['entity_span'].apply(pd.Series)\n",
    "    span.columns = ['begin', 'end']\n",
    "    \n",
    "    # merge back together\n",
    "    span_comp = span.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "    cols_to_keep = ['text', 'begin', 'end', 'brat_id', 'case']\n",
    "    span_comp = span_comp[cols_to_keep]\n",
    "    \n",
    "    return df, span, comp, span_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relations from .ann df\n",
    "def get_gold_relations(df, span_comp):\n",
    "    \"\"\"\n",
    "    extract BRAT relationships and associated entities\n",
    "    \"\"\"\n",
    "    \n",
    "    # get non-null relationship entities\n",
    "    comp_r = df[df.relation_entities.notnull()]\n",
    "    cols_to_keep = ['relation_type', 'relation_entities', 'brat_id', 'case']\n",
    "    comp_r = comp_r[cols_to_keep]\n",
    "\n",
    "    # split span into two separate columns \n",
    "    span_r = comp_r['relation_entities'].apply(pd.Series)\n",
    "    span_r.columns = ['entity1_r', 'entity2_r']\n",
    "\n",
    "    # merge back together\n",
    "    span_comp_r = span_r.merge(comp_r, how='inner', left_index=True, right_index=True)\n",
    "    cols_to_keep = ['relation_type', 'relation_entities','entity1_r', 'entity2_r', 'case']\n",
    "    span_comp_r = span_comp_r[cols_to_keep]\n",
    "\n",
    "    # get entities for relationship \n",
    "    \n",
    "    # left entity\n",
    "    entity_1r = span_comp_r.merge(span_comp, how='left', left_on=['entity1_r', 'case'], right_on=['brat_id', 'case'])\n",
    "    entity_1r = entity_1r.rename(columns={'text': 'text_entity1', 'begin': 'entity1_begin', 'end': 'entity1_end', 'brat_id': 'entity1', 'case_y': 'case'})\n",
    "    \n",
    "    # right entity\n",
    "    entity_2r = span_comp_r.merge(span_comp, how='left', left_on=['entity2_r', 'case'], right_on=['brat_id', 'case'])\n",
    "    entity_2r = entity_2r.rename(columns={'text': 'text_entity2', 'begin': 'entity2_begin', 'end': 'entity2_end', 'brat_id': 'entity2', 'case_y': 'case'})\n",
    "\n",
    "    # merge entities for relationship\n",
    "    entity_relation = entity_1r.merge(entity_2r, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    cols_to_keep = ['relation_type_x', 'entity1_r_x', 'entity2_r_x', 'text_entity1', 'entity1_begin', 'entity1_end', 'entity1',\n",
    "                   'text_entity2', 'entity2_begin', 'entity2_end', 'entity2', 'case_x']\n",
    "    entity_relation = entity_relation[cols_to_keep]\n",
    "    entity_relation = entity_relation.rename(columns={'relation_type_x': 'relation_type', 'entity1_r_x': 'entity1_r', 'entity2_r_x': 'entity2_r', 'case_x': 'case'})\n",
    "\n",
    "    return entity_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from .ann df\n",
    "def get_gold_attributes(df, span_comp):\n",
    "    \"\"\"\n",
    "    extract BRAT attributes and associated entities \n",
    "    \"\"\"\n",
    "    comp_a = df[df.attribute_entity.notnull()]\n",
    "    cols_to_keep = ['attribute_type', 'attribute_entity', 'attribute_degree', 'brat_id', 'case']\n",
    "    comp_a = comp_a[cols_to_keep]\n",
    "    entity_a = comp_a.merge(span_comp, how='left', left_on=['attribute_entity','case'], right_on=['brat_id', 'case'])\n",
    "    entity_a = entity_a.rename(columns={'case_x': 'case', 'brat_id_x': 'brat_id'})\n",
    "    cols_to_keep = ['attribute_type', 'attribute_entity', 'attribute_degree', 'brat_id', 'text']\n",
    "    entity_a = entity_a[cols_to_keep]\n",
    "   \n",
    "    return entity_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(df, ann_type):\n",
    "    \"\"\"\n",
    "    convert system or gold entity dataframe to list\n",
    "    ann_type for control of conversion\n",
    "    \"\"\"\n",
    "    data_out = []\n",
    "    for row in df.itertuples(index=True):\n",
    "        d = {'case': getattr(row, 'case'), \n",
    "             'begin': getattr(row, 'begin'), \n",
    "             'end': getattr(row, 'end'), \n",
    "             'text': getattr(row, 'text')}\n",
    "        if ann_type == 'system':\n",
    "            d['system'] = getattr(row, 'system')\n",
    "            d['type'] = getattr(row, 'type')\n",
    "        elif ann_type == 'gold':\n",
    "            d['gold_entity_type'] = getattr(row, 'entity_type')\n",
    "        \n",
    "        if d not in data_out:\n",
    "            data_out.append(d)\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-occurences \n",
    "def get_cooccurences(gold_out, system_out, nested = True):\n",
    "    \"\"\"\n",
    "    get coocurences between system and gold; nested -> no shared boundaries\n",
    "    \"\"\"\n",
    "    class Coocurences(object):\n",
    "        def __init__(self):\n",
    "            self.gold_system_match = 0\n",
    "            self.gold_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.gold_n = 0\n",
    "            self.matches = []\n",
    "            self.false_negatives = []\n",
    "            \n",
    "    c = Coocurences()\n",
    "    \n",
    "    for g in gold_out:\n",
    "        g_begin = int(g['begin'])\n",
    "        g_end = int(g['end'])\n",
    "        g_case = g['case']\n",
    "        mMatch = False\n",
    "        for s in system_out:\n",
    "            s_begin = int(s['begin'])\n",
    "            s_end = int(s['end'])\n",
    "            s_case = s['case']\n",
    "            if nested and g_case == s_case:\n",
    "                if (((s_begin >= g_begin and s_end < g_end) or \n",
    "                     (s_begin > g_begin and s_end <= g_end) or \n",
    "                     (g_begin >= s_begin and g_end < s_end) or \n",
    "                     (g_begin > s_begin and g_end <= s_end)) and \n",
    "                    ((s, g) not in c.matches)): \n",
    "                    if len(s['text']) <= 2*len(g['text']): \n",
    "                        c.matches.append((s, g))\n",
    "                        mMatch = True\n",
    "                        break\n",
    "        \n",
    "            if not nested and g_case == s_case:\n",
    "                if (((s_begin >= g_begin and s_begin < g_end and s_end > g_end) or \n",
    "                     (s_begin < g_begin and s_end > g_begin and s_end < g_end) or \n",
    "                     (s_begin < g_begin and s_end > g_begin and s_end > g_end)) and # or \n",
    "                    ((s, g) not in c.matches)): \n",
    "                    if len(s['text']) <= 2*len(g['text']): \n",
    "                        c.matches.append((s, g))\n",
    "                        mMatch = True\n",
    "                        break\n",
    "        \n",
    "        # no match, so increment for FN count\n",
    "        if mMatch == False:\n",
    "            c.gold_only += 1\n",
    "            \n",
    "            # use for bic analysis\n",
    "            fn = {'case': None, \n",
    "                  'begin': None, \n",
    "                  'end': None, \n",
    "                  'text': None, \n",
    "                  'system': None, \n",
    "                  'type': None}\n",
    "            \n",
    "            c.false_negatives.append((fn, g))\n",
    "    \n",
    "    # use for metrics \n",
    "    c.gold_system_match = len(c.matches)\n",
    "    c.system_only = len(system_out) - len(c.matches)\n",
    "    c.system_n = len(system_out)\n",
    "    c.gold_n = len(gold_out)\n",
    "    \n",
    "    # sanity check\n",
    "    if len(gold_out) - c.gold_system_match < 0:\n",
    "        print(c.matches)\n",
    "    \n",
    "    return c \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(system_only, gold_only, gold_system_match, system_n):\n",
    "    \"\"\"\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    class Metrics(object):\n",
    "        \"\"\"\n",
    "        metrics class \n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            self = self    \n",
    "            self.system_only = system_only\n",
    "            self.gold_only = gold_only\n",
    "            self.gold_system_match = gold_system_match\n",
    "            self.system_n = system_n\n",
    "            \n",
    "        def get_confusion_metrics(self):\n",
    "            \"\"\"\n",
    "            compute confusion matrix measures, as per  \n",
    "            https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "            \"\"\"\n",
    "            TP = self.gold_system_match\n",
    "            FP = self.system_only\n",
    "            FN = self.gold_only\n",
    "            TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "            confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            c = np.asarray(confusion)\n",
    "            recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "            precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            # Tignanelli Metric\n",
    "            if FN == 0:\n",
    "                TP_FN_R = TP\n",
    "            elif FN > 0:\n",
    "                TP_FN_R = TP/FN\n",
    "            \n",
    "            return F, recall, precision, TP, FP, FN, TP_FN_R, TM\n",
    "            \n",
    "    \n",
    "    return Metrics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity test\n",
    "\n",
    "#recall = TP/(FN + TP)\n",
    "\n",
    "#precision = TP/(FP + TP)\n",
    "#F = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "#print('F-score: {0}, precision: {1}, recall: {2}'.format(F, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all CAS XMI and BRAT ann files for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_data(cs, case_config, run_all=True):\n",
    "    \"\"\"\n",
    "    get data based on specified partition;\n",
    "    run_all controls whether both XMI CAS and BRAT are to be proceessed\n",
    "    \"\"\"\n",
    "    \n",
    "    gold_directory = cs.gold_path\n",
    "    cases, txt_directory, partition = case_config\n",
    "    gold_directory += txt_directory\n",
    "   \n",
    "    # named columns from fike\n",
    "    colnames=['brat_id', 'brat_mapping', 'text'] \n",
    "    # read files \n",
    "    gold_df = pd.DataFrame()\n",
    "    system_df = pd.DataFrame()\n",
    "    \n",
    "    system_out = [] # write sys_out\n",
    "    \n",
    "    # amicus partition is pared down to just amicus\n",
    "    if partition != 'amicus':\n",
    "        systems = cs.systems\n",
    "    else:\n",
    "        systems = cs.amicus\n",
    "    \n",
    "    i = 0\n",
    "    for case in cases: # ietrate through partitioned cases\n",
    "\n",
    "        if run_all: # used to control whther to skip creating system annotations\n",
    "            for system in systems:\n",
    "                system_directory = cs.system_path + system + '_out/'\n",
    "                system_file = system_directory + case + '-v1.txt.xmi'\n",
    "\n",
    "                with open(system_file, 'r') as f:\n",
    "                    lines = [x.strip() for x in f.readlines()]\n",
    "\n",
    "                # get sofa text    \n",
    "                text, xmi = system_sofa(lines)\n",
    "\n",
    "                # get system annotations as df and list\n",
    "                df, sys_out = get_system_annotations(xmi, system, case, text)\n",
    "                system_df = pd.concat([system_df, df], ignore_index=True)\n",
    "               \n",
    "                # save to list\n",
    "                system_out = sys_out + system_out\n",
    "                \n",
    "        # create dataframe of annotations from .ann \n",
    "        gold_file = gold_directory + case + '-v1.ann'\n",
    "        gold_temp = pd.read_table(gold_file, header=None, names=colnames)\n",
    "        gold_temp['case'] = case\n",
    "        gold_df = pd.concat([gold_df,gold_temp], ignore_index=True)\n",
    "    \n",
    "    return gold_df, system_df, system_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_out(cs, gold_out, system_out, case_config, nested = True):\n",
    "    \"\"\"\n",
    "    iterate over type system and gold annotation entities generating metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = pd.DataFrame()\n",
    "    cases, txt_directory, partition = case_config\n",
    "\n",
    "    # amicus partition is pared down to just amicus\n",
    "    if partition != 'amicus':\n",
    "        systems = cs.systems\n",
    "    else:\n",
    "        systems = cs.amicus\n",
    "    \n",
    "    # iterate through all specific systems\n",
    "    for sys in systems:\n",
    "        for ge in cs.gold_entities: # ieterate through specified entities\n",
    "            gold = []\n",
    "            for g in gold_out:\n",
    "                if g['gold_entity_type'] == ge:\n",
    "                    gold.append(g)\n",
    "            \n",
    "            types = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                system = [] \n",
    "                for s in system_out:\n",
    "                    if s['system'] == sys and s['type'] == t:\n",
    "                        system.append(s)\n",
    "                \n",
    "                c = get_cooccurences(gold, system, nested) # get matches, FN, etc.\n",
    "\n",
    "                if c.gold_system_match > 0: # compute confusion matrix metrics and write to deictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = get_metrics(c.system_only, c.gold_only, c.gold_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'entity': ge, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.gold_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "            \n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True) # needed due to duplicates in brat entity list!! TODO: remove\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all anotations\n",
    "# def get_gold_ann_data():\n",
    "\n",
    "#     import re, os, glob, path\n",
    "#     import pandas as pd\n",
    "\n",
    "#     directory_to_parse = cs.gold_path + '/all'\n",
    "#     os.chdir(directory_to_parse)\n",
    "\n",
    "#     test = pd.DataFrame()\n",
    "#     #for fname in glob.glob(\"/Volumes/GrenziData/development/nlp/nlpie/projects/trauma/brat_files/all/*.ann\"):\n",
    "#     for fname in glob.glob(directory_to_parse + '/*.ann'):\n",
    "#         # get filename and use for processed output filename\n",
    "#         t = os.path.basename(fname)\n",
    "#         u = t.split('.')[0]\n",
    "\n",
    "#         with open(fname) as f:\n",
    "#             colnames=['brat_id', 'brat_mapping', 'text'] \n",
    "#             # read in files\n",
    "#             temp = pd.read_table(f.name, header=None, names=colnames)\n",
    "#             temp['case'] = t.split('.')[0].split('-')[0]\n",
    "#         test = pd.concat([test,temp], ignore_index=True)\n",
    "    \n",
    "#     return test        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_annotations_out(cs, comp, entity_attributes, entity_relations, partition):\n",
    "    \"\"\"\n",
    "    write BRAT annotations gto Excel for analysis\n",
    "    \"\"\"\n",
    "    entities = comp['entity_type'].tolist()\n",
    "    entities = set(entities)\n",
    "    \n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_entities_'+ partition +'.xlsx')\n",
    "\n",
    "    for entity in sorted(list(entities)):\n",
    "        e = comp[comp['entity_type'] == entity]\n",
    "        e.to_excel(writer,sheet_name=entity, engine='xlsxwriter')\n",
    "\n",
    "    writer.save()\n",
    "    \n",
    "    attributes = entity_attributes['attribute_type'].tolist()\n",
    "    attributes = set(attributes)\n",
    "\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_attributes_'+ partition +'.xlsx')\n",
    "\n",
    "    for attribute in sorted(list(attributes)):\n",
    "        a = entity_attributes[entity_attributes['attribute_type'] == attribute]\n",
    "        a.to_excel(writer,attribute)\n",
    "    \n",
    "    writer.save()\n",
    "    \n",
    "    relationships = entity_relations['relation_type'].tolist()\n",
    "    relationships = set(relationships)\n",
    "    \n",
    "    writer = pd.ExcelWriter(cs.output_path + '/brat_annotated_relationships_'+ partition + '.xlsx')\n",
    "\n",
    "    for relation in sorted(list(relationships)):\n",
    "        r = entity_relations[entity_relations['relation_type'] == relation]\n",
    "        r.to_excel(writer,relation)\n",
    "    \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_txt(cs, case_config):\n",
    "    \"\"\"\n",
    "    get gold text for writing to JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    test = pd.DataFrame()\n",
    "    cases = []\n",
    "\n",
    "    cases, txt_directory, partition = case_config\n",
    "    \n",
    "    for fname in glob.glob(cs.txt_path + txt_directory + '*.txt'):\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0]\n",
    "        \n",
    "        with open(fname) as f:\n",
    "                text = f.read()\n",
    "                d = [text] \n",
    "                temp = pd.DataFrame(d, columns=['text'])\n",
    "                temp['case'] = t.split('.')[0].split('-')[0]\n",
    "                temp = temp.rename(columns={'0': 'text'})\n",
    "        cases.append(t.split('.')[0].split('-')[0])\n",
    "        text_out = pd.concat([text_out,temp], ignore_index=True)\n",
    "     \n",
    "    return text_out, cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations_to_json(span_comp, comp, gold_text, cases):\n",
    "    \"\"\"\n",
    "    BRAT annotations to JSON as per Prodigy spec\n",
    "    \"\"\"\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'entity_type']\n",
    "\n",
    "    span = span_comp.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "    span = span.rename(columns={'case_x': 'case'})\n",
    "    span = span[cols_to_keep]\n",
    "\n",
    "    #cases = [] TODO: use CaseSystem case_config object\n",
    "\n",
    "    for case in cases:\n",
    "        mask = span['case'] == case \n",
    "        df = span[mask]\n",
    "\n",
    "        mask = gold_text['case'] == case\n",
    "        txt = gold_text[mask]\n",
    "\n",
    "        for index, row in txt.iterrows():\n",
    "            out_text = row[0]\n",
    "            out_text = re.sub('\\|',' ', out_text)\n",
    "\n",
    "        keyDict = {\"spans\"}\n",
    "        s = dict([(key, []) for key in keyDict])\n",
    "\n",
    "        s[\"text\"] = out_text\n",
    "        for index, row in df.iterrows():\n",
    "            start = row[0]\n",
    "            end = row[1]\n",
    "            label = row[3]\n",
    "            d = {\"start\": int(start), \"end\": int(end), \"label\": label}\n",
    "            s['spans'].append(d)\n",
    "\n",
    "    print(json.dumps([s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_patterns_to_json(cs, comp, case_config):\n",
    "    \"\"\" \n",
    "    BRAT annotations to JSONL and Excel\n",
    "    \"\"\"\n",
    "    cases, txt_directory, partition = case_config\n",
    "        \n",
    "    patterns = set()\n",
    "    for case in cases:\n",
    "\n",
    "        mask = comp['case'] == case \n",
    "        mp = comp[mask]\n",
    "\n",
    "        cols_to_keep = ['text', 'entity_type']\n",
    "        mp= mp[cols_to_keep]\n",
    "\n",
    "        f = open(cs.output_path + '/match_patterns.jsonl', 'a')\n",
    "\n",
    "        # add to set for unique pattern\n",
    "        for index, row in mp.iterrows():\n",
    "            d = {\"text\": row[0], \"label\": row[1]} # use for seed terms\n",
    "            pattern = {\"label\": row[1], \"pattern\": row[0]}\n",
    "            patterns.add(json.dumps(pattern))\n",
    "        \n",
    "    for p in sorted(patterns):\n",
    "        #print(p)\n",
    "        f.write(p + '\\n')\n",
    "    \n",
    "    # output to spreadsheet by entity\n",
    "    cols_to_keep = ['entity_type', 'text']\n",
    "    out = comp[cols_to_keep]\n",
    "    entities = out['entity_type'].tolist()\n",
    "    entities = set(entities)\n",
    "\n",
    "    writer = pd.ExcelWriter(cs.output_path + '/annotated_entities_for_patterns.xlsx')\n",
    "\n",
    "    for entity in sorted(list(entities)):\n",
    "        e = out[out['entity_type'] == entity]\n",
    "        e.drop_duplicates(keep='last', inplace=True)\n",
    "        e = e.sort_values(by=['text'])\n",
    "        e.to_excel(writer,sheet_name=entity,columns=['text'],index=False)\n",
    "    \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_case(case):\n",
    "    with open(case) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "# test\n",
    "# read_case('/Users/gms/development/nlp/nlpie/projects/trauma/cases/pilot.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization object for reading in BRAT and XMI CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseSystem(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    cases by pilot, training and validation sets\n",
    "    paths by output, gold and system locations\n",
    "    directories by partition case types\n",
    "    extensions bu gold and system\n",
    "    systems \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        self.amicus = ['amicus']\n",
    "        self.pilot_directory = '/pass_one/'\n",
    "        self.new_directory = '/new_data/'\n",
    "        self.training_directory = '/training_set/'\n",
    "        self.validation_directory = '/validation_set/'\n",
    "        self.amicus_directory = '/adapt_amicus/'\n",
    "        self.all_directory = '/all/'\n",
    "        self.output_path = '/Users/gms/development/nlp/nlpie/projects/trauma/output'\n",
    "        self.gold_path = '/Users/gms/development/nlp/nlpie/projects/trauma/brat_files'\n",
    "        self.txt_path = '/Users/gms/development/nlp/nlpie/projects/trauma/txt_files'\n",
    "        self.system_path = '/Users/gms/development/nlp/nlpie/data/adapt-pass_one/'\n",
    "        self.gold_ftype = '/*.ann'\n",
    "        self.system_ftype = '/*.txt'\n",
    "        \n",
    "        '''\n",
    "        partitioned cases to be read into list\n",
    "        '''\n",
    "        self.case_directory = '/Users/gms/development/nlp/nlpie/projects/trauma/cases/'\n",
    "        self.pilot_set = 'pilot.txt'\n",
    "        self.training_set = 'training.txt'\n",
    "        self.validation_set = 'validation.txt'\n",
    "        self.new_set = 'new.txt'\n",
    "        self.test_set = 'test.txt'\n",
    "        \n",
    "        self.gold_entities = ['Age',\n",
    "                         'AirbagPresence',\n",
    "                         'DriverPassengerStatus',\n",
    "                         'EjectFromCar',\n",
    "                         'Entrapment',\n",
    "                         'Extricationtime',\n",
    "                         'Gender',\n",
    "                         'HeadOn',\n",
    "                         'IndicationProcedure', \n",
    "                         'InsuranceStatus',\n",
    "                         'LocationIntrusion',\n",
    "                         'OtherMinor',\n",
    "                         'OtherSevere',\n",
    "                         'Procedure',\n",
    "                         'Rollover',\n",
    "                         'SeatbeltPresence',\n",
    "                         'SeverityIntrusion',\n",
    "                         'TBone',\n",
    "                         'VehicleSpeed']\n",
    "        \n",
    "        self.gold_amicus_entities = ['Age',\n",
    "                         'AirbagPresence',\n",
    "                         'DriverPassengerStatus',\n",
    "                         'HeadOn',\n",
    "                         'IndicationProcedure', \n",
    "                         'InsuranceStatus',\n",
    "                         'OtherMinor',\n",
    "                         'Procedure',\n",
    "                         'SeatbeltPresence',\n",
    "                         'SeverityIntrusion',\n",
    "                         'TBone',\n",
    "                         'VehicleSpeed']\n",
    "        \n",
    "    # default config to training   \n",
    "    def case_config(self, partition='training'):\n",
    "        txt_directory = ''\n",
    "        \n",
    "        # read in cases by specified partition \n",
    "        def read_case(cases):\n",
    "            with open(cases) as f:\n",
    "                lines = f.read().splitlines()\n",
    "            return lines\n",
    "            \n",
    "        if partition == 'pilot':\n",
    "            txt_directory +=  self.pilot_directory\n",
    "            cases = read_case(self.case_directory + self.pilot_set)\n",
    "        elif partition == 'test':\n",
    "            txt_directory = None\n",
    "            cases = read_case(self.case_directory + self.test_set)\n",
    "        elif partition == 'training':\n",
    "            txt_directory += self.training_directory\n",
    "            cases = read_case(self.case_directory + self.training_set)\n",
    "        elif partition == 'validation':\n",
    "            txt_directory += self.validation_directory\n",
    "            cases = read_case(self.case_directory + self.validation_set)\n",
    "        elif partition == 'all':\n",
    "            txt_directory += self.all_directory\n",
    "            cases = read_case(self.case_directory + self.training_set) + read_case(self.case_directory + self.validation_set)\n",
    "        elif partition == 'amicusall': # run against 4-system output/full set\n",
    "            txt_directory += self.amicus_directory\n",
    "            cases = list(set(read_case(self.case_directory + self.training_set) + read_case(self.case_directory + self.validation_set)) - set(read_case(self.case_directory + self.pilot_set)))\n",
    "        elif partition == 'amicus': # run against amicus munged set\n",
    "            txt_directory += self.amicus_directory\n",
    "            cases = list(set(read_case(self.case_directory + self.training_set) + read_case(self.case_directory + self.validation_set)) - set(read_case(self.case_directory + self.pilot_set)))\n",
    "        elif partition == 'new':\n",
    "            txt_directory += self.new_directory\n",
    "            cases = read_case(self.case_directory + self.new_set)\n",
    "            \n",
    "        return cases, txt_directory, partition\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cs():\n",
    "    cs = CaseSystem()\n",
    "    cases, txt_directory, partition = cs.case_config('new')\n",
    "\n",
    "    print(len(cases))\n",
    "    print(txt_directory)\n",
    "\n",
    "test_cs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_system_annotation(m, tp = True):\n",
    "    \"\"\"\n",
    "    parse and return set of TP or FN annotations\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i in m:\n",
    "\n",
    "        if tp:\n",
    "            d = {'case':i[0]['case'],'sys text':i[0]['text'],'sys':i[0]['system'],'sys type': i[0]['type'],\n",
    "                 'sys begin':i[0]['begin'], 'sys end': i[0]['end'],'gold entity':i[1]['gold_entity_type'],\n",
    "                 'gold begin':i[1]['begin'], 'gold end': i[1]['end'], 'gold text':i[1]['text']}\n",
    "        else:\n",
    "            d = {'case':i[1]['case'],'sys text': 'FN','sys': None,'sys type': None,\n",
    "                 'sys begin': None, 'sys end': None,'gold entity':i[1]['gold_entity_type'],'gold begin':i[1]['begin'], \n",
    "                 'gold end': i[1]['end'], 'gold text':i[1]['text']}\n",
    "        \n",
    "        temp = pd.DataFrame(d, index=[0])\n",
    "        \n",
    "        data = pd.concat([temp, data], ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_in_class():\n",
    "    best ='''biomedicus Number Age\n",
    "ctakes Sentence AirbagPresence\n",
    "ctakes FractionAnnotation DriverPassengerStatus\n",
    "biomedicus IndefiniteQuantifierCue EjectFromCar\n",
    "biomedicus OtherAcronymSense Entrapment\n",
    "ctakes Predicate HeadOn\n",
    "ctakes SignSymptomMention IndicationProcedure\n",
    "metamap Candidate InsuranceStatus\n",
    "ctakes Predicate OtherMinor\n",
    "biomedicus OtherAcronymSense OtherSevere\n",
    "ctakes RomanNumeralAnnotation Procedure\n",
    "biomedicus Acronym Procedure\n",
    "ctakes Predicate Rollover\n",
    "metamap Phrase SeatbeltPresence\n",
    "ctakes SignSymptomMention SeatbeltPresence\n",
    "biomedicus IndefiniteQuantifierCue SeverityIntrusion\n",
    "biomedicus Acronym TBone\n",
    "biomedicus IndefiniteQuantifierCue VehicleSpeed\n",
    "'''.replace('\\n',' ').split(' ')\n",
    "\n",
    "    best_of = list()\n",
    "\n",
    "    i = 0\n",
    "    for b in best:\n",
    "        if i == 0:\n",
    "            d = {'system': b}\n",
    "        elif i == 1:\n",
    "            d['type'] = b\n",
    "        elif i == 2:\n",
    "            d['entity'] = b\n",
    "        i += 1\n",
    "        if i == 3:\n",
    "            i = 0\n",
    "            best_of.append(d)\n",
    "\n",
    "    return best_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_mean(metrics, cs):\n",
    "    \"\"\"\n",
    "    1. Group by entity type\n",
    "    2. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    3. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "    for g in cs.gold_entities: # traverse entity groups to compute\n",
    "        df = metrics[metrics['entity'] == g]\n",
    "\n",
    "        df['F1 rank']=df['F'].rank(ascending=0,method='average')\n",
    "        df['TP/FN rank']=df['TP/FN'].rank(ascending=0,method='average')\n",
    "        df['TM rank']=df['TM'].rank(ascending=0,method='average')\n",
    "        df['Gmean'] = gmean(df.iloc[:,-3:],axis=1)\n",
    "\n",
    "        frames = [data, df]\n",
    "        data = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Task: parse xmi, brat cases and do \"stuff\" with data\n",
    "def main():\n",
    "    \n",
    "    cs = CaseSystem()\n",
    "    start = time.time()\n",
    "    rtype = int(input(\"Run: 1->generate data; 2->brat ann out; 3->patterns out; 4->metrics; 5->top n bic w/ TP/FN; 6->amicus mash; 7->metrics\"))\n",
    "    \n",
    "    nested = False # used to control type of overlapping match\n",
    "    nest = 'system_coverage'\n",
    "    \n",
    "    partition = 'pilot'\n",
    "    #partition = 'amicus'\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        # option 1: run evaluation on ADAPT versus brat\n",
    "        brat, system, system_out = get_evaluation_data(cs, cs.case_config(partition), run_all=True)\n",
    "        \n",
    "        # write to output to save time!\n",
    "        system.to_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        \n",
    "        print('end get eval data', (time.time() - start))\n",
    "        brat, span, comp, span_comp = get_gold_annotations(brat)\n",
    "        \n",
    "        print('end get gold annotations', (time.time() - start))\n",
    "\n",
    "        gold = span.merge(comp, how='inner', left_index=True, right_index=True)\n",
    "        gold.to_csv(cs.output_path + '/gold_out'+ partition + '.csv')\n",
    "        \n",
    "        cols_to_keep = ['case', 'begin', 'end', 'text', 'entity_type']\n",
    "        \n",
    "        gold_out = df_to_list(gold, 'gold')\n",
    "        \n",
    "        # 'nested' sets the type of span coverage: True: gold covers system; False: system covers gold\n",
    "        m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), nested)\n",
    "        \n",
    "        print('end metrics out', (time.time() - start))\n",
    "\n",
    "        print(m.head())\n",
    "        # get geometric mean of ranked averages of F1, TP/FN, TM\n",
    "        metrics = geometric_mean(m, cs)\n",
    "        \n",
    "        print('end geometric mean', (time.time() - start))\n",
    "        metrics.to_csv(cs.output_path + '/test_metrics'+ partition + nest + '.csv')\n",
    "        \n",
    "        print(gold.head())\n",
    "        print(system.head())\n",
    "    \n",
    "    elif (rtype == 2):\n",
    "        # option 2: write brat annotations to Excel\n",
    "        brat, system, system_out = get_evaluation_data(cs, cs.case_config(partition), run_all=False)\n",
    "        brat, span, comp, span_comp = get_gold_annotations(brat)\n",
    "        entity_relations = get_gold_relations(brat, span_comp)\n",
    "        entity_attributes = get_gold_attributes(brat, span_comp)\n",
    "        all_annotations_out(cs, comp, entity_attributes, entity_relations, partition)\n",
    "   \n",
    "    elif (rtype == 3):\n",
    "        # option 3: get annotated JSON, matching patterns -> NER label\n",
    "        brat, system, system_out = get_evaluation_data(cs, cs.case_config('training'), run_all=False)\n",
    "        brat, span, comp, span_comp = get_gold_annotations(brat)\n",
    "        gold_text, cases = get_gold_txt(cs, cs.case_config('training'))   \n",
    "        annotations_to_json(span_comp, comp, gold_text, cases)\n",
    "        annotated_patterns_to_json(cs, comp, cs.case_config('training'))\n",
    "    \n",
    "    elif (rtype == 4):\n",
    "        # run geometric mean\n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics.csv')\n",
    "        \n",
    "        geometric_mean(data, cs)\n",
    "        \n",
    "        geometric_mean(data, cs).to_csv(cs.output_path + '/bic_ranking.csv')\n",
    "    \n",
    "    elif (rtype == 5):\n",
    "        n = input('Select top n:')\n",
    "        print('Processing top' + n + ' for best system types:')\n",
    "\n",
    "        # get system -> brat annotations from disk to save processing time\n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        best_in_class = []\n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics'+ partition + nest +'.csv')\n",
    "        \n",
    "        bic = pd.DataFrame()\n",
    "        for g in cs.gold_entities: # traverse enitities for generating bic metrics\n",
    "            filter_metrics = data[data['entity'] == g]\n",
    "            filter_metrics = filter_metrics.sort_values(by=['Gmean']).head(int(n))\n",
    "            cols_to_keep = ['system', 'type', 'entity']\n",
    "            top_n = filter_metrics[cols_to_keep]\n",
    "            for index, row in top_n.iterrows():\n",
    "                best_in_class.append({'system':row['system'],'type': row['type'], 'entity': row['entity']})\n",
    "                \n",
    "            frames = [bic, filter_metrics]\n",
    "            bic = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "        writer = pd.ExcelWriter(cs.output_path + '/best_in_class_'+ partition +'_draft.xlsx')\n",
    "\n",
    "        bic.to_excel(writer,sheet_name='Top ' + n + ' sys annotations')\n",
    "        \n",
    "        bic = pd.DataFrame()\n",
    "        for b in best_in_class: # write bic metrics to csv\n",
    "            gold = ann[ann['entity_type'] == b['entity']]\n",
    "            system = sys[(sys['system'] == b['system']) & (sys['type'] == b['type'])]\n",
    "            \n",
    "            g1 = df_to_list(gold, 'gold')\n",
    "            s1 = df_to_list(system, 'system')\n",
    "            c = get_cooccurences(g1, s1)\n",
    "\n",
    "            tp_ann = gold_system_annotation(c.matches)\n",
    "            fn_ann = gold_system_annotation(c.false_negatives, False)\n",
    "            \n",
    "            frames = [tp_ann, fn_ann]\n",
    "            out = pd.concat(frames, ignore_index=True)\n",
    "            frames = [bic, out]\n",
    "            bic = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "        bic.to_csv(cs.output_path + '/best_in_class_annotations_top_' + n + '_' + partition +'.csv')\n",
    "\n",
    "        for ge in cs.gold_entities: # to sheet by entitiy\n",
    "            b = bic[bic['gold entity'] == ge]\n",
    "            if not b.empty:\n",
    "                b.to_excel(writer,sheet_name=ge)\n",
    "            \n",
    "        writer.save()\n",
    "    \n",
    "    elif (rtype == 6):\n",
    "        # get system -> brat annotations for specified lists of entities and system types \n",
    "        # used for bic analysis\n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        entity_of_interest =  ['SeverityIntrusion'] #['IndicationProcedure']\n",
    "        best_in_class = []\n",
    "        \n",
    "        data = pd.read_csv(cs.output_path + '/test_metrics'+ partition + nest +'.csv')\n",
    "        \n",
    "        filter_metrics = pd.DataFrame()\n",
    "        for g in entity_of_interest:\n",
    "            filter_metrics = data[(data['entity'] == g) & \n",
    "                        (data['type'] == 'Sentence')]\n",
    "#             filter_metrics = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'IndefiniteQuantifierCue') |  \n",
    "#                          (data['type'] == 'StandaloneQuantifier') |\n",
    "#                          (data['type'] == 'Number'))]\n",
    "#             filter_metrics = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'SignSymptomMention') |  \n",
    "#                          (data['type'] == 'UmlsConcept'))]\n",
    "#             filter_metrics = data[(data['entity'] == g) & \n",
    "#                         ((data['type'] == 'Sentence') |  \n",
    "#                          (data['type'] == 'Phrase'))]\n",
    "            \n",
    "        \n",
    "            cols_to_keep = ['system', 'type', 'entity']\n",
    "            top = filter_metrics[cols_to_keep]\n",
    "            for index, row in top.iterrows():\n",
    "                best_in_class.append({'system':row['system'],'type': row['type'], 'entity': row['entity']})\n",
    "                \n",
    "            frames = [temp, filter_metrics]\n",
    "            temp = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "            print(best_in_class)\n",
    "        \n",
    "        writer = pd.ExcelWriter(cs.output_path + '/best_in_class_all_'+ partition + ' ' + entity_of_interest[0] + nest +'.xlsx')\n",
    "\n",
    "        temp.to_excel(writer,sheet_name='All sys annotations', engine='openpyxl')\n",
    "        \n",
    "        # only choose those from pilot eval\n",
    "        #chosen_best = ['IndefiniteQuantifierCue','StandaloneQuantifier','Number'] #SeverityIntrusion -> gold covefage\n",
    "        #chosen_best = ['SignSymptomMention','UmlsConcept'] # IndicationProcedure -> gold coverage\n",
    "        #chosen_best = ['Sentence','Phrase'] # IndicationProcedure -> sys coverage\n",
    "        chosen_best = ['Sentence'] # SeverityIntrusion -> sys coverage\n",
    "        \n",
    "        bic_ann = pd.DataFrame()\n",
    "        for b in best_in_class:\n",
    "            #print(b)\n",
    "            if b['type'] in chosen_best:\n",
    "                gold = ann[ann['entity_type'] == b['entity']]\n",
    "                system = sys[(sys['system'] == b['system']) & (sys['type'] == b['type'])]\n",
    "\n",
    "                g1 = df_to_list(gold, 'gold')\n",
    "                s1 = df_to_list(system, 'system')\n",
    "                c = get_cooccurences(g1, s1)\n",
    "\n",
    "                tp_ann = gold_system_annotation(c.matches)\n",
    "                \n",
    "                print(temp.head(10))\n",
    "\n",
    "                fn_ann = gold_system_annotation(c.false_negatives, False)\n",
    "\n",
    "                frames = [tp_ann, fn_ann]\n",
    "                out = pd.concat(frames, ignore_index=True)\n",
    "                frames = [bic_ann, out]\n",
    "                bic_ann = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "        temp_ann.to_csv(cs.output_path + '/best_in_class_annotations_' + partition +'.csv')\n",
    "\n",
    "        for ge in entity_of_interest:\n",
    "            t = bic_ann[bic_ann['gold entity'] == ge]\n",
    "            if not t.empty:\n",
    "                t.to_excel(writer,sheet_name=ge)\n",
    "            \n",
    "        writer.save()\n",
    "\n",
    "    elif (rtype == 7):\n",
    "        # print metrics\n",
    "        \n",
    "        sys = pd.read_csv(cs.output_path + '/system_out'+ partition +'.csv')\n",
    "        ann = pd.read_csv(cs.output_path + '/gold_out'+ partition +'.csv')\n",
    "        \n",
    "        gold_out = df_to_list(ann, 'gold')\n",
    "        system_out = df_to_list(sys, 'system')\n",
    "        \n",
    "        m = metrics_out(cs, gold_out, system_out, cs.case_config(partition), True)\n",
    "\n",
    "        print(m.head())\n",
    "        metrics = geometric_mean(m, cs)\n",
    "        print(metrics.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_partitions():\n",
    "    cs = CaseSystem()\n",
    "    cases, txt_directory, partition = cs.case_config('pilot')\n",
    "    \n",
    "    #print(cases)\n",
    "    \n",
    "    p = cases\n",
    "    print(len(p))\n",
    "    \n",
    "    cases, txt_directory, partition = cs.case_config('amicusall')\n",
    "    \n",
    "    #print(cases)\n",
    "    \n",
    "    v = cases\n",
    "    print(len(v))\n",
    "   \n",
    "    print(len(set(p).union(set(v))))\n",
    "    print(len(set(p).intersection(set(v))))\n",
    "    \n",
    "\n",
    "    \n",
    "validate_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(data):\n",
    "    l = analyzer(data)\n",
    "    return ['_'.join(y.split()) for y in l]\n",
    "\n",
    "def semantic_phrases():\n",
    "    \"\"\"\n",
    "    method for generatin semantic lists \n",
    "    NBL need to run remotely, due to w2p model containing PHI\n",
    "    \"\"\"\n",
    "    \n",
    "    # use for creation of n-grams\n",
    "    # https://stackoverflow.com/questions/13423919/computing-n-grams-using-python\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2)) # use for n-gram generation\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "    partition = 'amicusall'\n",
    "    output_path = '/home/gms/projects/trauma/data'\n",
    "    threshold = 0.5\n",
    "      \n",
    "    sys = pd.read_csv(output_path + '/system_out'+ partition +'.csv')\n",
    "    ann = pd.read_csv(output_path + '/gold_out'+ partition +'.csv')\n",
    "    phrase = KeyedVectors.load_word2vec_format(datapath(\"/home/gms/projects/word2vec/fairview-vectors-c5_2010-2014-phrase1.bin\"), binary=True)\n",
    "    \n",
    "    writer = pd.ExcelWriter('semantic_similarity_112_cases-new_terms.xlsx')\n",
    "    \n",
    "    # ADAPT study terms on IndicationProcedure\n",
    "    terms = ['unresponsive',\n",
    "            'unconscious',\n",
    "            'agonal',\n",
    "            'hypotensive',\n",
    "            'tachycardic',\n",
    "            'diminished_breath_sounds',\n",
    "            'absent_breath_sounds',\n",
    "            'breath_sounds',\n",
    "            'desaturation',\n",
    "            'cpr',\n",
    "            'massive_hemorrhage',\n",
    "            'entrapment']\n",
    "\n",
    "    # Future research on Procedue entity\n",
    "    # terms =['intubation',\n",
    "    # 'et_tube',\n",
    "    # 'iv',\n",
    "    # 'io',\n",
    "    # 'prbc',\n",
    "    # 'transfusion',\n",
    "    # 'txa',\n",
    "    # 'lucas',\n",
    "    # 'extrication',\n",
    "    # 'igel',\n",
    "    # 'airway',\n",
    "    # 'intraosseous',\n",
    "    # 'intravenous_access',\n",
    "    # 'tranexamic', \n",
    "    # 'bolus']\n",
    "\n",
    "    def get_gold_similarities():\n",
    "        \"\"\"\n",
    "        get list of synonymous terms from gold standard annotations by entity of interest\n",
    "        \"\"\"\n",
    "        mask = ann['entity_type'] == 'IndicationProcedure' \n",
    "        d = []\n",
    "        gold = ann[mask].sort_values(by=['case'])\n",
    "        for index, row in gold.iterrows():\n",
    "            str = get_ngrams(row['text'])\n",
    "            for s in str:\n",
    "                if s in phrase.vocab: #ensure token is in model, other wise error is thrown\n",
    "                    for term in terms:\n",
    "                        if term in phrase.vocab:\n",
    "                            if phrase.similarity(term, s) > threshold:\n",
    "                                d.append({'term': term, 'begin': row['begin'], 'end': row['end'],  'case': row['case'], 'token': s, 'entity_type': row['entity_type'], 'sentence': row['text'], 'cos distance': phrase.similarity(term, s)})\n",
    "        g = pd.DataFrame(d)\n",
    "        g.to_excel(writer,sheet_name='fvphrases_gold_w2v_hits', engine='xlsxwriter')\n",
    "        g.to_csv('fvphrases_gold.csv')\n",
    "\n",
    "    def get_sys_similarities():\n",
    "        \"\"\"\n",
    "        get list of synonymous terms from system annotations by bic annotations for entity of itnerest\n",
    "        \"\"\"\n",
    "        mask = (((sys['system'] == 'ctakes') & (sys['type'] == 'SignSymptomMention')) | ((sys['system'] == 'biomedicus') & (sys['type'] == 'UmlsConcept')) | ((sys['system'] == 'metamap') & (sys['type'] == 'Phrase')) | ((sys['system'] == 'ctakes') & (sys['type'] == 'Sentence')) | ((sys['system'] == 'clamp') & (sys['type'] == 'Sentence')))\n",
    "        d = []\n",
    "        ref = sys[mask].sort_values(by=['case'])\n",
    "        for index, row in ref.iterrows():\n",
    "            str = get_ngrams(row['text'])\n",
    "            for s in str:\n",
    "                if s in phrase.vocab:\n",
    "                    for term in terms:\n",
    "                        if term in phrase.vocab:\n",
    "                            if phrase.similarity(term, s) > threshold:\n",
    "                                d.append({'term': term, 'begin': row['begin'], 'end': row['end'], 'system': row['system'], 'type': row['type'], 'case': row['case'], 'token': s, 'sentence': row['text'], 'cos distance': phrase.similarity(term, s)})\n",
    "        s = pd.DataFrame(d)\n",
    "        s.to_excel(writer,sheet_name='fvphrases_sys_w2v_hits', engine='xlsxwriter')\n",
    "        s.to_csv('fvphrases_sys.csv')\n",
    "\n",
    "    get_gold_similarities()\n",
    "    get_sys_similarities()\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv = KeyedVectors.load_word2vec_format(datapath(\"/Users/gms/Downloads/GoogleNews-vectors-negative300.bin\"), binary=True)\n",
    "#pmc = KeyedVectors.load_word2vec_format(datapath(\"/Users/gms/Downloads/GoogleNews-vectors-negative300.bin\"), binary=True)\n",
    "#phrase = KeyedVectors.load_word2vec_format(datapath(\"/home/gms/projects/word2vec/fairview-vectors-c5_2010-2014-phrase1.bin\"), binary=True)\n",
    "#pmc = KeyedVectors.load_word2vec_format(datapath(\"/home/gms/projects/word2vec/pmc-vectors_lc.bin\"), binary=True)\n",
    "#fv = KeyedVectors.load_word2vec_format(datapath(\"/home/gms/projects/word2vec/fairview-vectors-c5.bin\"), binary=True)\n",
    "#phrase = KeyedVectors.load_word2vec_format(datapath(\"/home/gms/projects/word2vec/fairview-vectors-c5_2010-2014-phrase1.bin\"), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):  \n",
    "    \"\"\"\n",
    "    from https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "    \"\"\"\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annotation_lists(cs, case_config):\n",
    "    \n",
    "    cases, txt_directory, partition = case_config\n",
    "    sys = pd.read_csv(cs.output_path + '/fvphrases_sys.csv')\n",
    "    gold = pd.read_csv(cs.output_path + '/fvphrases_gold.csv')\n",
    "    print(cases)\n",
    "\n",
    "    for case in cases:  \n",
    "        sys = sys[sys['case'] == case]\n",
    "        gold = gold[(gold['case']  == case) & (gold['entity_type'] == 'IndicationProcedure')]\n",
    "        sys_d = dict()\n",
    "        gold_d = dict()\n",
    "        sys_l = list()\n",
    "        gold_l = list()\n",
    "\n",
    "        # build system annotation list by case\n",
    "        for index, row in sys.iterrows():\n",
    "            if sys_d.get('case_annotation') is None: # define system key for determining LD \n",
    "                sys_d['case_annotation'] = {'case':row['case'],'system':row['system'],'type':row['type'],'token':row['token'],'term':row['term']}\n",
    "                sys_l.append(sys_d['case_annotation'])\n",
    "            else:\n",
    "                sys_d['case_annotation'] = {'case':row['case'],'system':row['system'],'type':row['type'],'token':row['token'],'term':row['term']}\n",
    "                if sys_d['case_annotation'] not in sys_l:\n",
    "                    sys_l.append(sys_d['case_annotation'])\n",
    "\n",
    "        # build gold annotation list by case\n",
    "        for index, row in gold.iterrows():\n",
    "            if gold_d.get('case_annotation') is None: # define gold key for comparison to system key for determing LD\n",
    "                gold_d['case_annotation'] = {'case':row['case'],'token':row['token'],'term':row['term']}\n",
    "                gold_l.append(gold_d['case_annotation'])\n",
    "            else:\n",
    "                gold_d['case_annotation'] = {'case':row['case'],'token':row['token'],'term':row['term']}\n",
    "                if gold_d['case_annotation'] not in gold_l:\n",
    "                    gold_l.append(gold_d['case_annotation'])\n",
    "\n",
    "        return gold_l, sys_l\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CaseSystem()\n",
    "gold_l, sys_l = build_annotation_lists(cs, cs.case_config('test'))\n",
    "\n",
    "def get_min_ld(gold_l, sys_l):\n",
    "    \"\"\"\n",
    "    iterate over system annotations to find gold annotation with min LD\n",
    "    \"\"\" \n",
    "    b = []\n",
    "    for i in sys_l:\n",
    "        best_score = 99999\n",
    "        if i not in b:\n",
    "            # https://stackoverflow.com/questions/41806076/accessing-items-from-a-frozenset-in-python\n",
    "            key = frozenset(i.items())\n",
    "            b.append({key:()})\n",
    "        for j in gold_l:\n",
    "            temp_score = levenshtein(i['token'],j['token'])\n",
    "            if temp_score < best_score:\n",
    "                best_score = temp_score\n",
    "                for d in b:\n",
    "                    # https://stackoverflow.com/questions/4291236/edit-the-values-in-a-list-of-dictionaries\n",
    "                    d.update((k, (j, best_score)) for k, v in d.items() if k == key)\n",
    "\n",
    "    for k in b:\n",
    "        for key, value in k.items():\n",
    "            print (dict(key), value)\n",
    "            \n",
    "get_min_ld(gold_l, sys_l)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
